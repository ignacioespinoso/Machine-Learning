{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Before Running\n",
    "Add files health.txt, bags.csv and word2vec.csv to the health-dataset folder.\n",
    "\n",
    "# Introduction\n",
    "Here, we'll explore k-means clustering algorithm applied to health news in twitter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cluster import AffinityPropagation\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.preprocessing import normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_health_data():\n",
    "    return pd.read_csv('health-dataset/health.txt', sep='|')\n",
    "def load_word2vec_data():\n",
    "    return pd.read_csv('health-dataset/word2vec.csv')\n",
    "def load_bags_data():\n",
    "    return pd.read_csv('health-dataset/bags.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we load our data and set the amount of clusters for K-means clustering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_health_df = load_word2vec_data()\n",
    "bags_health_df = load_bags_data()\n",
    "clusters_amount=100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First attempt at word2vec\n",
    "First we'll get the results using the given word2vec.\n",
    "Using KMeans we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.03299543980949137"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmeans = KMeans(n_clusters=clusters_amount).fit(word2vec_health_df)\n",
    "labels = kmeans.labels_\n",
    "metrics.silhouette_score(word2vec_health_df, labels, metric='euclidean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using affinity propagation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "affinity_prop = AffinityPropagation().fit(word2vec_health_df)\n",
    "labels = affinity_prop.labels_\n",
    "silhouette_score(word2vec_health_df, labels, metric='euclidean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First attempt at bags\n",
    "Apply the same logic to bags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "kmeans = KMeans(n_clusters=clusters_amount).fit(bags_health_df)\n",
    "labels = kmeans.labels_\n",
    "silhouette_score(bags_health_df, labels, metric='euclidean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using affinity propagation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "affinity_prop = AffinityPropagation().fit(bags_health_df)\n",
    "labels = affinity_prop.labels_\n",
    "silhouette_score(bags_health_df, labels, metric='euclidean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now normalizing our input:\n",
    "Repeating the previous procedures by normalizing the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_health_df_normalized = normalize(word2vec_health_df)\n",
    "kmeans = KMeans(n_clusters=clusters_amount).fit(word2vec_health_df_normalized)\n",
    "labels = kmeans.labels_\n",
    "metrics.silhouette_score(word2vec_health_df_normalized, labels, metric='euclidean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using affinity propagation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "affinity_prop = AffinityPropagation().fit(word2vec_health_df_normalized)\n",
    "labels = affinity_prop.labels_\n",
    "silhouette_score(word2vec_health_df_normalized, labels, metric='euclidean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for bags:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bags_health_df_normalized = normalize(bags_health_df)\n",
    "kmeans = KMeans(n_clusters=clusters_amount).fit(word2vec_health_df_normalized)\n",
    "labels = kmeans.labels_\n",
    "metrics.silhouette_score(word2vec_health_df_normalized, labels, metric='euclidean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using affinity propagation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "affinity_prop = AffinityPropagation().fit(bags_health_df_normalized)\n",
    "labels = affinity_prop.labels_\n",
    "silhouette_score(bags_health_df_normalized, labels, metric='euclidean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Changing strings to remove useless words\n",
    "Opening and first look at the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['publish_date', 'headline_text']\n"
     ]
    }
   ],
   "source": [
    "health_df = load_health_data()\n",
    "health_df = health_df.drop(['id'], axis=1)\n",
    "print(list(health_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete words without meaningful information for our context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in health_df.iterrows():\n",
    "    word_list = row['headline_text'].split()\n",
    "    deleted_words=[\"RT\", \"a\", \"are\", \"it\", \"the\", \"she\", \"you\", \"of\", \"to\", \"that's\", \"-\", \"on\", \"I\", \"he\"]\n",
    "    row['headline_text'] = ' '.join([word for word in word_list if word not in deleted_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
