{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "Here, we'll explore neural networks applied to the fashion-mnist problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import csv\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions\n",
    "\n",
    "def formatArray (dataFrame, columnToExtract) :\n",
    "    array = dataFrame.values\n",
    "    target = array[:,columnToExtract]\n",
    "    params = np.delete(array, columnToExtract, axis = 1)\n",
    "    return params, target\n",
    "\n",
    "def loadFashionTrainData():\n",
    "    return pd.read_csv(\"fashion-mnist-dataset/fashion-mnist_train.csv\")\n",
    "\n",
    "def loadFashionTestData():\n",
    "    return pd.read_csv(\"fashion-mnist-dataset/fashion-mnist_test.csv\")\n",
    "\n",
    "def split_train_test(data, test_ratio):\n",
    "    np.random.seed(42)\n",
    "    shuffled_indices = np.random.permutation(len(data))\n",
    "    test_set_size = int(len(data) * test_ratio)\n",
    "    test_indices = shuffled_indices[:test_set_size]\n",
    "    train_indices = shuffled_indices[test_set_size:]\n",
    "    return data.iloc[train_indices], data.iloc[test_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The dataset\n",
    "First and foremost, we'll open train and test data. The training data is split to obtain validation items and the the target values are also separated from the original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "fashionTrainDataset = loadFashionTrainData()\n",
    "fashionTestDataset = loadFashionTestData()\n",
    "fashionTrain, fashionValidation = split_train_test(fashionTrainDataset, 0.2)\n",
    "fashionTrain, fashionTarget = formatArray(fashionTrain, 0)\n",
    "\n",
    "print (fashionTrain[:5])\n",
    "print (type(fashionTrain))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation and Softmax Functions\n",
    "First, we'll start by implementing some useful functions seen in class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(n):\n",
    "    return (1/(1+exp(-n)))\n",
    "\n",
    "def derivative_sigmoid(n):\n",
    "    x = sigmoid(n)\n",
    "    return x * (1 - x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(n):\n",
    "    if n < 0:\n",
    "        return 0\n",
    "    return n\n",
    "\n",
    "def derivative_relu(n):\n",
    "    if n < 0:\n",
    "        return 0\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leaky_relu(n):\n",
    "    if n > 0:\n",
    "        return n\n",
    "    return 0.01 * n\n",
    "\n",
    "def derivative_leaky_relu(n):\n",
    "    if n < 0:\n",
    "        return 0.01\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(n):\n",
    "    return np.exp(n)/ np.sum(np.exp(x), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Propagation\n",
    "In this section, we define forward propagation related functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_1hl(input_dimension,hidden_layer_1_neurons, output_dimension):\n",
    "    neural_data = {}\n",
    "    np.random.seed(0)\n",
    "    neural_data['w1'] = np.random.randn(input_dimension, hidden_layer_1_neurons)/ np.sqrt(input_dimension)\n",
    "    neural_data['w2'] = np.random.randn(hidden_layer_1_neurons, output_dimension)/ np.sqrt(hidden_layer_1_neurons)\n",
    "    \n",
    "    neural_data['b1'] = np.zeros((1, hidden_layer_1_neurons))\n",
    "    neural_data['b2'] = np.zeros((1, output_dimension))\n",
    "    return neural_data\n",
    "\n",
    "def initialize_2hl(input_dimension, hidden_layer_1_neurons, hidden_layer_2_neurons, output_dimension):\n",
    "    neural_data = {}\n",
    "    np.random.seed(0)\n",
    "\n",
    "    neural_data['w1'] = np.random.randn(input_dimension, hidden_layer_1_neurons)/ np.sqrt(input_dimension)\n",
    "    neural_data['w2'] = np.random.randn(hidden_layer_1_neurons, hidden_layer_2_neurons)/ np.sqrt(hidden_layer_1_neurons)\n",
    "    neural_data['w3'] = np.random.randn(hidden_layer_2_neurons, output_dimension)/ np.sqrt(hidden_layer_2_neurons)\n",
    "\n",
    "    neural_data['b1'] = np.zeros((1, hidden_layer_1_neurons))\n",
    "    neural_data['b2'] = np.zeros((1, hidden_layer_2_neurons))\n",
    "    neural_data['b3'] = np.zeros((1, output_dimension))\n",
    "    return neural_data\n",
    "    \n",
    "def forward_prop_1hl(x, neural_data):\n",
    "    w1 , w2, b1, b2 = neural_data['w1'], neural_data['w2'], neural_data['b1'], neural_data['b2']\n",
    "    x1 = np.dot(x, w1) + b1 #Output of hidden layer\n",
    "    y1 = [relu(n) for n in x1] #Output of hidden layer with activation function\n",
    "    x2 = np.dot(y1, w2) + b2 #Output of last layer\n",
    "    neural_data['x1'] = x1\n",
    "    neural_data['x2'] = x2\n",
    "    neural_data['y1'] = y1\n",
    "    neural_data['o'] = softmax(y2)\n",
    "    return neural_data['o']\n",
    "\n",
    "def forward_prop_2hl(x, neural_data):\n",
    "    w1 , w2, w3, b1, b2, b3 = neural_data['w1'], neural_data['w2'], neural_data['w3'], neural_data['b1'], neural_data['b2'], neural_data['b3']\n",
    "\n",
    "    x1 = np.dot(x, w1) + b1\n",
    "    y1 = [relu(n) for n in x1]\n",
    "    x2 = np.dot(a1, w2) + b2\n",
    "    y2 = [relu(n) for n in x2]\n",
    "    x3 = np.dot(a2, w3) + b3\n",
    "    \n",
    "    neural_data['x1'] = x1\n",
    "    neural_data['x2'] = x2\n",
    "    neural_data['x3'] = x3\n",
    "\n",
    "    neural_data['y1'] = y1\n",
    "    neural_data['y2'] = y2\n",
    "    \n",
    "    neural_data['o'] = softmax(x3)\n",
    "    return neural_data['o']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction Functions\n",
    "Helper functions that return predictions, given our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_1hl(x, neural_data):\n",
    "    return np.argmax(forward_prop_1hl(x,neural_data))\n",
    "\n",
    "def predict_2hl(x, neural_data):\n",
    "    return np.argmax(forward_prop_2hl(x,neural_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_cost(fashionTrainOutput, fashionTarget, fashionTargetMinusOne, testCasesAmount):\n",
    "    final_cost = 0\n",
    "    for j in testCasesAmount:\n",
    "        cost = np.add(np.dot(fashionTarget, np.log10(fashionTrainOutput[j])),np.dot(fashionTargetMinusOne, (np.ones(10) - np.log10(fashionTrainOutput[j]))))\n",
    "    final_cost = np.sum(cost)/testCasesAmount\n",
    "    return final_cost\n",
    "\n",
    "def total_error(fashionTrainOutput, fashionTarget):\n",
    "    error = 0\n",
    "    for i in len()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and backward propagation\n",
    "\n",
    "Some notes:\n",
    "\n",
    "### For one hidden layer:\n",
    "- x1 = #Output of hidden layer\n",
    "- x2 = #Output of last layer\n",
    "- y1 = #Output of hidden layer with activation function\n",
    "- o = Final output with Softmax\n",
    "\n",
    "### For TWO hidden layers:\n",
    "- x1 = #Output of first hidden layer\n",
    "- x2 = #Output of second hidden layer\n",
    "- x3 = Output of last layer\n",
    "- y1 = #Output of first hidden layer with activation function\n",
    "- y2 = #Output of second hidden layer with activation function\n",
    "- o = Final output with Softmax\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One hidden layer\n",
    "\n",
    "Here, we present our code and results achieved by a learning algorithm that uses a neural network with only one hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_neural_network_1hl(input_dimension, hidden_layer_1_neurons, output_dimension, input_data, epochs):\n",
    "#     Initializes weights and biases for our neural network\n",
    "    neural_data = initialize_1hl(neural_data, hidden_layer_1_neurons, output_dimension)\n",
    "    sigmoid = True\n",
    "    relu = False\n",
    "    leaky_relu = False\n",
    "\n",
    "#     Performs Backpropagation\n",
    "    for i in range(epochs):\n",
    "        \n",
    "    ##\n",
    "    ##\n",
    "    ##   TODO: ADD MINI-BATCH ALGORITHM TO GENERATE MINI-BATCH DATA\n",
    "    ##\n",
    "    ##\n",
    "    ##\n",
    "#         Performs Forward propagation\n",
    "        probs = forward_prop_1hl(mini_batch_data, neural_data)\n",
    "    \n",
    "#         Performs Backward propagation\n",
    "        delta3 = probs - miniBatchTarget\n",
    "        dW2 = (neural_data['y1'].T).dot(delta3)\n",
    "        db2 = np.sum(delta3, axis=0, keepdims=True)\n",
    "        delta2 = delta3.dot(neural_data['w2'].T)\n",
    "        aux = neural_data['y1']\n",
    "        if sigmoid: \n",
    "            for i in aux:\n",
    "                i = derivative_sigmoid(i)\n",
    "        if relu:\n",
    "            for i in aux:\n",
    "                i = derivative_relu(i)\n",
    "        if leaky_relu:\n",
    "            for i in aux:\n",
    "                i = derivative_leaky_relu(i)        \n",
    "        delta2 = delta2 * aux\n",
    "        dW1 = np.dot(mini_batch_data.T, delta2)\n",
    "        db1 = np.sum(delta2, axis=0)\n",
    "        \n",
    "        \n",
    "         # Performs regularization\n",
    "        dW2 += regularization_rate * neural_data['w2']\n",
    "        dW1 += regularization_rate * neural_data['w1']\n",
    "\n",
    "        # Gradient descent parameter update\n",
    "        neural_data['w1'] += -learning_rate * dW1\n",
    "        neural_data['b1'] += -learning_rate * db1\n",
    "        neural_data['w2'] += -learning_rate * dW2\n",
    "        neural_data['b2'] += -learning_rate * db2\n",
    "        \n",
    "        print(\"Ended iteration\", i)\n",
    "        \n",
    "    return neural_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Two hidden layers\n",
    "\n",
    "Same as before, but for 2 hidden layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_neural_network_2hl(input_dimension, hidden_layer_1_neurons, hidden_layer_2_neurons, output_dimension, input_data, epochs):\n",
    "#     Initializes weights and biases for our neural network\n",
    "    neural_data = initialize_1hl(neural_data, hidden_layer_1_neurons, output_dimension)\n",
    "    sigmoid = True\n",
    "    relu = False\n",
    "    leaky_relu = False\n",
    "\n",
    "#     Performs Backpropagation\n",
    "    for i in range(epochs):\n",
    "        \n",
    "    ##\n",
    "    ##\n",
    "    ##   TODO: ADD MINI-BATCH ALGORITHM TO GENERATE MINI-BATCH DATA\n",
    "    ##\n",
    "    ##\n",
    "    ##\n",
    "#         Performs Forward propagation\n",
    "        probs = forward_prop_1hl(mini_batch_data, neural_data)\n",
    "    \n",
    "#         Performs Backward propagation\n",
    "        delta4 = probs - miniBatchTarget\n",
    "        dW3 = (neural_data['y2'].T).dot(delta4)\n",
    "        db3 = np.sum(delta4, axis=0, keepdims=True)\n",
    "        delta3 = delta4.dot(neural_data['w3'].T)\n",
    "        aux = neural_data['y2']\n",
    "        if sigmoid: \n",
    "            for i in aux:\n",
    "                i = derivative_sigmoid(i)\n",
    "        if relu:\n",
    "            for i in aux:\n",
    "                i = derivative_relu(i)\n",
    "        if leaky_relu:\n",
    "            for i in aux:\n",
    "                i = derivative_leaky_relu(i)        \n",
    "        delta3 = delta3 * aux\n",
    "        dW2 = np.dot(mini_batch_data.T, delta3)\n",
    "        db2 = np.sum(delta3, axis=0)\n",
    "        delta2 = delta3.dot(neural_data['w2'].T)\n",
    "        aux = neural_data['y1']\n",
    "        if sigmoid: \n",
    "            for i in aux:\n",
    "                i = derivative_sigmoid(i)\n",
    "        if relu:\n",
    "            for i in aux:\n",
    "                i = derivative_relu(i)\n",
    "        if leaky_relu:\n",
    "            for i in aux:\n",
    "                i = derivative_leaky_relu(i)\n",
    "        delta2 = delta2 * aux\n",
    "        dW1 = np.dot(mini_batch_data.T, delta2)\n",
    "        db1 = np.sum(delta2, axis=0)      \n",
    "        \n",
    "         # Performs regularization\n",
    "        dW3 += regularization_rate * neural_data['w3']\n",
    "        dW2 += regularization_rate * neural_data['w2']\n",
    "        dW1 += regularization_rate * neural_data['w1']\n",
    "\n",
    "        # Gradient descent parameter update\n",
    "        \n",
    "        neural_data['w1'] += -learning_rate * dW1\n",
    "        neural_data['b1'] += -learning_rate * db1\n",
    "        neural_data['w2'] += -learning_rate * dW2\n",
    "        neural_data['b2'] += -learning_rate * db2\n",
    "        neural_data['w3'] += -learning_rate * dW3\n",
    "        neural_data['b3'] += -learning_rate * db3\n",
    "        \n",
    "        print(\"Ended iteration\", i)\n",
    "        \n",
    "    return neural_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the neural networks\n",
    "\n",
    "Now, we'll test our neural networks under multiple circumstances on the validation set, so we can gest the best possible models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem details\n",
    "- Input dimension: 28x28 = 784 neurons\n",
    "- Output dimension: 10 classes = 10 neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_layers = 1\n",
    "input_neurons = 784\n",
    "hidden_layer_1_neurons = 15\n",
    "hidden_layer_2_neurons = 15\n",
    "learning_rate = 1\n",
    "regularization_rate = 0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
