{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "Here, we'll explore neural networks applied to the fashion-mnist problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import csv\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions\n",
    "\n",
    "def formatArray (dataFrame, columnToExtract) :\n",
    "    array = dataFrame.values\n",
    "    target = array[:,columnToExtract]\n",
    "    params = np.delete(array, columnToExtract, axis = 1)\n",
    "    return params, target\n",
    "\n",
    "def loadFashionTrainData():\n",
    "    return pd.read_csv(\"fashion-mnist-dataset/fashion-mnist_train.csv\")\n",
    "\n",
    "def loadFashionTestData():\n",
    "    return pd.read_csv(\"fashion-mnist-dataset/fashion-mnist_test.csv\")\n",
    "\n",
    "def split_train_test(data, test_ratio):\n",
    "    np.random.seed(42)\n",
    "    shuffled_indices = np.random.permutation(len(data))\n",
    "    test_set_size = int(len(data) * test_ratio)\n",
    "    test_indices = shuffled_indices[:test_set_size]\n",
    "    train_indices = shuffled_indices[test_set_size:]\n",
    "    return data.iloc[train_indices], data.iloc[test_indices]\n",
    "\n",
    "def createTarget (target):\n",
    "    results = np.zeros((target.size, 10), dtype=int)\n",
    "    for i in range(10):\n",
    "        for j in range(target.size):\n",
    "            if (target[j] != i):\n",
    "                results[j][i - 1] = 0\n",
    "            else:\n",
    "                results[j][i - 1] = 1\n",
    "    return results\n",
    "\n",
    "def p_print(a):\n",
    "    for x in a:\n",
    "        print(*x, sep=\" \")\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The dataset\n",
    "First and foremost, we'll open train and test data. The training data is split to obtain validation items and the the target values are also separated from the original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "fashionTrainDataset = loadFashionTrainData()\n",
    "fashionTestDataset = loadFashionTestData()\n",
    "fashionTrain, fashionValidation = split_train_test(fashionTrainDataset, 0.2)\n",
    "fashionTrainParams, fashionTrainTarget = formatArray(fashionTrain, 0)\n",
    "fashionTrainParams = fashionTrainParams/255\n",
    "fashionValidationParams, fashionValidationTarget = formatArray(fashionValidation, 0)\n",
    "fashionValidationTarget = createTarget(fashionValidationTarget)\n",
    "fashionValidationParams = fashionValidationParams/255\n",
    "print (fashionTrainParams[:5])\n",
    "print (type(fashionTrainParams))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation and Softmax Functions\n",
    "First, we'll start by implementing some useful functions seen in class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(n):\n",
    "    return (1/(1+np.exp(-n)))\n",
    "\n",
    "def derivative_sigmoid(n):\n",
    "    x = sigmoid(n)\n",
    "    return x * (1 - x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(n):\n",
    "    if n < 0:\n",
    "        return 0\n",
    "    return n\n",
    "\n",
    "def derivative_relu(n):\n",
    "    if n < 0:\n",
    "        return 0\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leaky_relu(n):\n",
    "    if n > 0:\n",
    "        return n\n",
    "    return 0.01 * n\n",
    "\n",
    "def derivative_leaky_relu(n):\n",
    "    if n < 0:\n",
    "        return 0.01\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(n):\n",
    "    exp = np.exp(n)\n",
    "    test_sum = np.sum(exp, axis=1, keepdims=True)\n",
    "    return exp/test_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Propagation\n",
    "In this section, we define forward propagation related functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_1hl(input_dimension,hidden_layer_1_neurons, output_dimension):\n",
    "    neural_data = {}\n",
    "    np.random.seed()\n",
    "    neural_data['w1'] = np.random.randn(input_dimension, hidden_layer_1_neurons)/ np.sqrt(input_dimension)\n",
    "    neural_data['w2'] = np.random.randn(hidden_layer_1_neurons, output_dimension)/ np.sqrt(input_dimension)\n",
    "    \n",
    "    neural_data['b1'] = np.zeros((1, hidden_layer_1_neurons))\n",
    "    neural_data['b2'] = np.zeros((1, output_dimension))\n",
    "    return neural_data\n",
    "\n",
    "def initialize_2hl(input_dimension, hidden_layer_1_neurons, hidden_layer_2_neurons, output_dimension):\n",
    "    neural_data = {}\n",
    "    np.random.seed(0)\n",
    "\n",
    "    neural_data['w1'] = np.random.randn(input_dimension, hidden_layer_1_neurons)/ np.sqrt(input_dimension)\n",
    "    neural_data['w2'] = np.random.randn(hidden_layer_1_neurons, hidden_layer_2_neurons)/ np.sqrt(hidden_layer_1_neurons)\n",
    "    neural_data['w3'] = np.random.randn(hidden_layer_2_neurons, output_dimension)/ np.sqrt(hidden_layer_2_neurons)\n",
    "\n",
    "    neural_data['b1'] = np.zeros((1, hidden_layer_1_neurons))\n",
    "    neural_data['b2'] = np.zeros((1, hidden_layer_2_neurons))\n",
    "    neural_data['b3'] = np.zeros((1, output_dimension))\n",
    "    return neural_data\n",
    "    \n",
    "def forward_prop_1hl(x, neural_data, activation=\"sigmoid\"):\n",
    "    w1 , w2, b1, b2 = neural_data['w1'], neural_data['w2'], neural_data['b1'], neural_data['b2']\n",
    "    x1 = np.dot(x, w1) + b1 #Output of hidden layer\n",
    "    if activation == \"sigmoid\":\n",
    "        y1 = np.asarray([[sigmoid(n) for n in j] for j in x1]) #Output of hidden layer with activation function\n",
    "    elif actication == \"relu\":\n",
    "        y1 = np.asarray([[relu(n) for n in j] for j in x1]) #Output of hidden layer with activation function\n",
    "    elif activation == \"leaky-relu\"\n",
    "        y1 = np.asarray([[leaky_relu(n) for n in j] for j in x1]) #Output of hidden layer with activation function\n",
    "    x2 = np.dot(y1, w2) + b2 #Output of last layer\n",
    "    neural_data['x1'] = x1\n",
    "    neural_data['x2'] = x2\n",
    "    neural_data['y1'] = y1\n",
    "    neural_data['o'] = softmax(x2)  # Final output with softmax\n",
    "\n",
    "    return neural_data['o']\n",
    "\n",
    "def forward_prop_2hl(x, neural_data):\n",
    "    w1 , w2, w3, b1, b2, b3 = neural_data['w1'], neural_data['w2'], neural_data['w3'], neural_data['b1'], neural_data['b2'], neural_data['b3']\n",
    "\n",
    "    x1 = np.dot(x, w1) + b1\n",
    "    y1 = np.asarray([[relu(n) for n in j] for j in x1])\n",
    "    x2 = np.dot(a1, w2) + b2\n",
    "    y2 = np.asarray([[relu(n) for n in j] for j in x2])\n",
    "    x3 = np.dot(a2, w3) + b3\n",
    "    \n",
    "    neural_data['x1'] = x1\n",
    "    neural_data['x2'] = x2\n",
    "    neural_data['x3'] = x3\n",
    "\n",
    "    neural_data['y1'] = y1\n",
    "    neural_data['y2'] = y2\n",
    "    \n",
    "    neural_data['o'] = softmax(x3)\n",
    "    return neural_data['o']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction Functions\n",
    "Helper functions that return predictions, given our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_1hl(x, neural_data):\n",
    "    test = forward_prop_1hl(x,neural_data)\n",
    "    return np.argmax(test, axis=1)\n",
    "\n",
    "def predict_2hl(x, neural_data):\n",
    "    return np.argmax(forward_prop_2hl(x,neural_data), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_cost(fashionTrainOutput, fashionTarget, testCasesAmount):\n",
    "    fashionTargetMinusOne = fashionTarget - 1\n",
    "    cost = 0\n",
    "    for j in range(testCasesAmount):\n",
    "        cost += np.add(np.multiply(fashionTarget, np.log10(fashionTrainOutput[j])),np.multiply(fashionTargetMinusOne, (1- np.log10(fashionTrainOutput[j]))))\n",
    "    cost = cost*(-1)/testCasesAmount\n",
    "    return cost\n",
    "\n",
    "def neuralNetworkCostFunction(fashionTrainOutput, fashionTarget):\n",
    "    diference = fashionTrainOutput - fashionTarget\n",
    "    squareDiference = diference ** 2\n",
    "    n = fashionTrainOutput.shape[0]    \n",
    "    return (np.sum(squareDiference)/(2*n))\n",
    "\n",
    "def regressionLogisticCostFunction (results, model, X):\n",
    "    agaTheta = model.predict_proba(X)\n",
    "    n = X.shape[0]\n",
    "    diference = results - agaTheta\n",
    "    squareDiference = diference * diference\n",
    "    return (np.sum(squareDiference)/(2*n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and backward propagation\n",
    "\n",
    "Some notes:\n",
    "\n",
    "### For one hidden layer:\n",
    "- x1 = #Output of hidden layer\n",
    "- x2 = #Output of last layer\n",
    "- y1 = #Output of hidden layer with activation function\n",
    "- o = Final output with Softmax\n",
    "\n",
    "### For TWO hidden layers:\n",
    "- x1 = #Output of first hidden layer\n",
    "- x2 = #Output of second hidden layer\n",
    "- x3 = Output of last layer\n",
    "- y1 = #Output of first hidden layer with activation function\n",
    "- y2 = #Output of second hidden layer with activation function\n",
    "- o = Final output with Softmax\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One hidden layer\n",
    "\n",
    "Here, we present our code and results achieved by a learning algorithm that uses a neural network with only one hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_neural_network_1hl(hidden_layer_1_neurons, epochs, trainParams, trainTarget, learning_rate, regularization_rate, activation=\"sigmoid\"):\n",
    "    print(\"Beginning training...\")\n",
    "    input_dimension = 784\n",
    "    output_dimension = 10\n",
    "#     Initializes weights and biases for our neural network\n",
    "    neural_data = initialize_1hl(input_dimension, hidden_layer_1_neurons, output_dimension)\n",
    "    print(\"Initialized weights\")\n",
    "    sigmoid = True\n",
    "    relu = False\n",
    "    leaky_relu = False\n",
    "    \n",
    "#     Prepares for mini-batch\n",
    "    batchSize = 600\n",
    "    start_idx = 0\n",
    "    indices = np.arange(trainParams.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "    print(\"Prepared for mini-batch.\")\n",
    "#     Performs Backpropagation\n",
    "    capitalDelta3 = 0\n",
    "    capitalDelta2 = 0\n",
    "    for j in range(epochs):\n",
    "        excerpt = indices[start_idx:start_idx + batchSize]\n",
    "        mini_batch_data = trainParams[excerpt]\n",
    "        miniBatchTarget = createTarget(trainTarget[excerpt])\n",
    "#         Performs Forward propagation\n",
    "        probs = forward_prop_1hl(mini_batch_data, neural_data, activation)\n",
    "    \n",
    "\n",
    "#         Performs Backward propagation\n",
    "\n",
    "        delta3 = probs - miniBatchTarget\n",
    "\n",
    "        dW2 =(1./batchSize)* (neural_data['y1'].T).dot(delta3)\n",
    "        db2 =(1./batchSize)* ( np.sum(delta3, axis=0, keepdims=True))\n",
    "        delta2 = np.dot(delta3, neural_data['w2'].T)\n",
    "        aux = neural_data['y1']\n",
    "        if activation == \"sigmoid\":\n",
    "            aux = [[derivative_sigmoid(n) for n in x] for x in aux]\n",
    "        if activation == \"relu\":\n",
    "            aux = [[derivative_relu(n) for n in x] for x in aux]\n",
    "        if activation == \"leaky_relu\":\n",
    "            aux = [[derivative_leaky_relu(n) for n in x] for x in aux]\n",
    "\n",
    "        delta2 = delta2 * aux\n",
    "\n",
    "        dW1 = (1./batchSize)*np.dot(mini_batch_data.T, delta2)\n",
    "        db1 = (1./batchSize)*np.sum(delta2, axis=0)\n",
    "        \n",
    "        \n",
    "#          # Performs regularization\n",
    "#         dW2 += regularization_rate * neural_data['w2']\n",
    "#         dW1 += regularization_rate * neural_data['w1']\n",
    "\n",
    "        # Gradient descent parameter update\n",
    "        neural_data['w1'] -= learning_rate * dW1\n",
    "        neural_data['b1'] -= learning_rate * db1\n",
    "        neural_data['w2'] -= learning_rate * dW2\n",
    "        neural_data['b2'] -= learning_rate * db2\n",
    "        \n",
    "        if j%50 == 0:\n",
    "            #         Calculates costs\n",
    "            cost = neuralNetworkCostFunction(probs, miniBatchTarget)\n",
    "            validation_probs = forward_prop_1hl(fashionValidationParams, neural_data)\n",
    "            validation_cost = neuralNetworkCostFunction(validation_probs, fashionValidationTarget)\n",
    "            print(\"Ended iteration\", j,\" Cost: \", cost, \" Validation cost: \", validation_cost)\n",
    "        start_idx += batchSize;\n",
    "        start_idx %= mini_batch_data.shape[0]\n",
    "    return neural_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Two hidden layers\n",
    "\n",
    "Same as before, but for 2 hidden layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_neural_network_2hl(hidden_layer_1_neurons, hidden_layer_2_neurons, epochs, trainParams, trainTarget, learning_rate, regularization_rate):\n",
    "    input_dimension = 784\n",
    "    output_dimension = 10\n",
    "#     Initializes weights and biases for our neural network\n",
    "    neural_data = initialize_2hl(input_dimension, hidden_layer_1_neurons, hidden_layer_2_neurons, output_dimension)\n",
    "    sigmoid = True\n",
    "    relu = False\n",
    "    leaky_relu = False\n",
    "    \n",
    "#     Prepares for mini-batch\n",
    "    batchSize = 64\n",
    "    start_idx = 0;\n",
    "    indices = np.arange(trainParams.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "    excerpt = indices[start_idx:start_idx + batchSize]\n",
    "    mini_batch_data = trainParams[excerpt]\n",
    "    miniBatchTarget = createTarget(trainTarget[excerpt])\n",
    "#     Performs Backpropagation\n",
    "    for j in range(epochs):\n",
    "\n",
    "#         Performs Forward propagation\n",
    "        probs = forward_prop_1hl(mini_batch_data, neural_data)\n",
    "        \n",
    "#         Calculates cost\n",
    "\n",
    "#         Performs Backward propagation\n",
    "        delta4 = probs - miniBatchTarget\n",
    "        dW3 = (neural_data['y2'].T).dot(delta4)\n",
    "        db3 = np.sum(delta4, axis=0, keepdims=True)\n",
    "        delta3 = delta4.dot(neural_data['w3'].T)\n",
    "        aux = neural_data['y2']\n",
    "        if sigmoid:\n",
    "            aux = [[derivative_sigmoid(n) for n in x] for x in aux]\n",
    "        if relu:\n",
    "            aux = [[derivative_relu(n) for n in x] for x in aux]\n",
    "        if leaky_relu:\n",
    "            aux = [[derivative_leaky_relu(n) for n in x] for x in aux]        \n",
    "        delta3 = delta3 * aux\n",
    "        dW2 = np.dot(mini_batch_data.T, delta3)\n",
    "        db2 = np.sum(delta3, axis=0)\n",
    "        delta2 = delta3.dot(neural_data['w2'].T)  #look for issues here\n",
    "        aux = neural_data['y1']\n",
    "        if sigmoid:\n",
    "            aux = [[derivative_sigmoid(n) for n in x] for x in aux]\n",
    "        if relu:\n",
    "            aux = [[derivative_relu(n) for n in x] for x in aux]\n",
    "        if leaky_relu:\n",
    "            aux = [[derivative_leaky_relu(n) for n in x] for x in aux]\n",
    "        delta2 = delta2 * aux\n",
    "        dW1 = np.dot(mini_batch_data.T, delta2)\n",
    "        db1 = np.sum(delta2, axis=0)      \n",
    "        \n",
    "#          # Performs regularization\n",
    "#         dW3 += regularization_rate * neural_data['w3']\n",
    "#         dW2 += regularization_rate * neural_data['w2']\n",
    "#         dW1 += regularization_rate * neural_data['w1']\n",
    "\n",
    "        # Gradient descent parameter update\n",
    "        \n",
    "        neural_data['w1'] += -learning_rate * dW1\n",
    "        neural_data['b1'] += -learning_rate * db1\n",
    "        neural_data['w2'] += -learning_rate * dW2\n",
    "        neural_data['b2'] += -learning_rate * db2\n",
    "        neural_data['w3'] += -learning_rate * dW3\n",
    "        neural_data['b3'] += -learning_rate * db3\n",
    "        \n",
    "        print(\"Ended iteration\", j)\n",
    "        start_idx += 1;\n",
    "        start_idx %= mini_batch_data.shape[0]\n",
    "        \n",
    "    return neural_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the neural networks\n",
    "\n",
    "Now, we'll test our neural networks under multiple circumstances on the validation set, so we can gest the best possible models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem details\n",
    "- Input dimension: 28x28 = 784 neurons\n",
    "- Output dimension: 10 classes = 10 neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_layers = 1\n",
    "input_neurons = 784\n",
    "output_neurons = 10\n",
    "hidden_layer_1_neurons = 50\n",
    "hidden_layer_2_neurons = 15\n",
    "learning_rate = 0.1\n",
    "regularization_rate = 0\n",
    "epochs = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting small\n",
    "First, we will train a model using a hidden layer with 50 neurons and 200 epochs, which is small given the input of 784 neurons. We will run 3 times for each activation function to get an average result (that depends heavily on the initialization of the weights)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sigmoid.50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning training...\n",
      "Initialized weights\n",
      "Prepared for mini-batch.\n",
      "Ended iteration 0  Cost:  0.45019556729150473  Validation cost:  0.44974519740812025\n",
      "Ended iteration 50  Cost:  0.38282042236201175  Validation cost:  0.38685795845508103\n",
      "Ended iteration 100  Cost:  0.2985541421094475  Validation cost:  0.3091462685935611\n",
      "Ended iteration 150  Cost:  0.2552539933612356  Validation cost:  0.26837535768752285\n",
      "======================First model trained=====================\n",
      "Beginning training...\n",
      "Initialized weights\n",
      "Prepared for mini-batch.\n",
      "Ended iteration 0  Cost:  0.4518269892624977  Validation cost:  0.4504125866956274\n",
      "Ended iteration 50  Cost:  0.3780817055795375  Validation cost:  0.37895947072367014\n",
      "Ended iteration 100  Cost:  0.3021440971255299  Validation cost:  0.3038535945384921\n",
      "Ended iteration 150  Cost:  0.26151065585507044  Validation cost:  0.2648871988094492\n",
      "======================Second model trained=====================\n",
      "Beginning training...\n",
      "Initialized weights\n",
      "Prepared for mini-batch.\n",
      "Ended iteration 0  Cost:  0.4506599232860998  Validation cost:  0.4497981826610876\n",
      "Ended iteration 50  Cost:  0.3879049116298812  Validation cost:  0.3888165548149526\n",
      "Ended iteration 100  Cost:  0.31422872935180796  Validation cost:  0.3180163198405954\n",
      "Ended iteration 150  Cost:  0.27040713095444374  Validation cost:  0.27612444005497394\n",
      "======================Third model trained=====================\n",
      "First model validation cost:  0.24794224259566058\n",
      "Second model validation cost:  0.2428568781828087\n",
      "Third model validation cost:  0.25340658894536866\n",
      "Average:  0.24806856990794598\n"
     ]
    }
   ],
   "source": [
    "epochs=200\n",
    "hidden_layer_1_neurons=50\n",
    "model_1hl_sigmoid1 = train_neural_network_1hl(hidden_layer_1_neurons, epochs, fashionTrainParams, fashionTrainTarget, learning_rate, regularization_rate)\n",
    "print(\"======================First model trained=====================\")\n",
    "model_1hl_sigmoid2 = train_neural_network_1hl(hidden_layer_1_neurons, epochs, fashionTrainParams, fashionTrainTarget, learning_rate, regularization_rate)\n",
    "print(\"======================Second model trained=====================\")\n",
    "model_1hl_sigmoid3 = train_neural_network_1hl(hidden_layer_1_neurons, epochs, fashionTrainParams, fashionTrainTarget, learning_rate, regularization_rate)\n",
    "print(\"======================Third model trained=====================\")\n",
    "probs_sigmoid1 = forward_prop_1hl(fashionValidationParams, model_1hl_sigmoid1)\n",
    "cost1 = neuralNetworkCostFunction(probs_sigmoid1, fashionValidationTarget)\n",
    "print(\"First model validation cost: \", cost1)\n",
    "probs_sigmoid2 = forward_prop_1hl(fashionValidationParams, model_1hl_sigmoid2)\n",
    "cost2 = neuralNetworkCostFunction(probs_sigmoid2, fashionValidationTarget)\n",
    "print(\"Second model validation cost: \", cost2)\n",
    "probs_sigmoid3 = forward_prop_1hl(fashionValidationParams, model_1hl_sigmoid3)\n",
    "cost3 = neuralNetworkCostFunction(probs_sigmoid3, fashionValidationTarget)\n",
    "avg = ((cost1+cost2+cost3)/3)\n",
    "print(\"Third model validation cost: \", cost3)\n",
    "print(\"Average: \", avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since everything seems ok, we'll increase the number of epochs to 1000, but mantaining the current amount of neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning training...\n",
      "Initialized weights\n",
      "Prepared for mini-batch.\n",
      "Ended iteration 0  Cost:  0.44959187390799116  Validation cost:  0.4480108001649867\n",
      "Ended iteration 50  Cost:  0.36952606683374717  Validation cost:  0.3740381977705004\n",
      "Ended iteration 100  Cost:  0.3080086989490664  Validation cost:  0.3166197600437621\n",
      "Ended iteration 150  Cost:  0.2739455816782257  Validation cost:  0.28392435323233045\n",
      "Ended iteration 200  Cost:  0.25346563044121545  Validation cost:  0.2647823077469343\n",
      "Ended iteration 250  Cost:  0.23911963300137548  Validation cost:  0.2518448704674362\n",
      "Ended iteration 300  Cost:  0.22794751997651863  Validation cost:  0.24198656413587208\n",
      "Ended iteration 350  Cost:  0.21922585148820797  Validation cost:  0.23433720963608956\n",
      "Ended iteration 400  Cost:  0.21222300597657845  Validation cost:  0.2282338261045932\n",
      "Ended iteration 450  Cost:  0.20613082997249557  Validation cost:  0.22341075882204434\n",
      "Ended iteration 500  Cost:  0.20091433358064  Validation cost:  0.21939940136607516\n",
      "Ended iteration 550  Cost:  0.19642391178047164  Validation cost:  0.2161454211943026\n",
      "Ended iteration 600  Cost:  0.19251166549455342  Validation cost:  0.21354585919863972\n",
      "Ended iteration 650  Cost:  0.18830518756736767  Validation cost:  0.21089095595850885\n",
      "Ended iteration 700  Cost:  0.1836933226880115  Validation cost:  0.20769425676395808\n",
      "Ended iteration 750  Cost:  0.1785377260298178  Validation cost:  0.20395074186919324\n",
      "Ended iteration 800  Cost:  0.1727937990160746  Validation cost:  0.20002180452356724\n",
      "Ended iteration 850  Cost:  0.1668821399091144  Validation cost:  0.19640703865740708\n",
      "Ended iteration 900  Cost:  0.16087745279231047  Validation cost:  0.19310981908560568\n",
      "Ended iteration 950  Cost:  0.15516086158994724  Validation cost:  0.19018741109634502\n",
      "======================First model trained=====================\n",
      "Beginning training...\n",
      "Initialized weights\n",
      "Prepared for mini-batch.\n",
      "Ended iteration 0  Cost:  0.4508657352259521  Validation cost:  0.44963530839395877\n",
      "Ended iteration 50  Cost:  0.3736013783199553  Validation cost:  0.3756691769928839\n",
      "Ended iteration 100  Cost:  0.29440511295200456  Validation cost:  0.3008821897255535\n",
      "Ended iteration 150  Cost:  0.25590363860967386  Validation cost:  0.26534341630071656\n",
      "Ended iteration 200  Cost:  0.23575295534863774  Validation cost:  0.24709460568496056\n",
      "Ended iteration 250  Cost:  0.22131543784691407  Validation cost:  0.23504773690982947\n",
      "Ended iteration 300  Cost:  0.2089150193261488  Validation cost:  0.22548855359723063\n",
      "Ended iteration 350  Cost:  0.19789258931657053  Validation cost:  0.21757822405676883\n",
      "Ended iteration 400  Cost:  0.1884143507570145  Validation cost:  0.21117520194223802\n",
      "Ended iteration 450  Cost:  0.18024742625770315  Validation cost:  0.206214685697717\n",
      "Ended iteration 500  Cost:  0.17314137000530797  Validation cost:  0.20222818781371116\n",
      "Ended iteration 550  Cost:  0.16662968364104272  Validation cost:  0.19864763715321537\n",
      "Ended iteration 600  Cost:  0.16026784890764817  Validation cost:  0.1954359708602807\n",
      "Ended iteration 650  Cost:  0.15403651475872907  Validation cost:  0.1924446594224816\n",
      "Ended iteration 700  Cost:  0.14835612089246597  Validation cost:  0.18973613937676656\n",
      "Ended iteration 750  Cost:  0.14310776920749285  Validation cost:  0.1873516309798014\n",
      "Ended iteration 800  Cost:  0.1382651492869628  Validation cost:  0.18526727519748767\n",
      "Ended iteration 850  Cost:  0.1336806203246553  Validation cost:  0.18347484078891402\n",
      "Ended iteration 900  Cost:  0.12936047571876658  Validation cost:  0.18195921706071544\n",
      "Ended iteration 950  Cost:  0.12536671653180928  Validation cost:  0.18070120287317293\n",
      "======================Second model trained=====================\n",
      "Beginning training...\n",
      "Initialized weights\n",
      "Prepared for mini-batch.\n",
      "Ended iteration 0  Cost:  0.4492254974105749  Validation cost:  0.4484908364130246\n",
      "Ended iteration 50  Cost:  0.3570719990026176  Validation cost:  0.3631848821901798\n",
      "Ended iteration 100  Cost:  0.2732023699296722  Validation cost:  0.2866527801797865\n",
      "Ended iteration 150  Cost:  0.2331382365285999  Validation cost:  0.2519773804929293\n",
      "Ended iteration 200  Cost:  0.21210072172872116  Validation cost:  0.2352732673037938\n",
      "Ended iteration 250  Cost:  0.1976578392130651  Validation cost:  0.2250130859751274\n",
      "Ended iteration 300  Cost:  0.18637995146149006  Validation cost:  0.21711268446838963\n",
      "Ended iteration 350  Cost:  0.1764367513524751  Validation cost:  0.21015610597649909\n",
      "Ended iteration 400  Cost:  0.16776396214472195  Validation cost:  0.20394229579463158\n",
      "Ended iteration 450  Cost:  0.1600881166749246  Validation cost:  0.19870298666462755\n",
      "Ended iteration 500  Cost:  0.15304686902440734  Validation cost:  0.1944432823029915\n",
      "Ended iteration 550  Cost:  0.14680575945573116  Validation cost:  0.19110849822761858\n",
      "Ended iteration 600  Cost:  0.14118986805816106  Validation cost:  0.18849185605344276\n",
      "Ended iteration 650  Cost:  0.1360541113399429  Validation cost:  0.18645847868732487\n",
      "Ended iteration 700  Cost:  0.1314567067620907  Validation cost:  0.18477942098325725\n",
      "Ended iteration 750  Cost:  0.12741454338413225  Validation cost:  0.18340745519027016\n",
      "Ended iteration 800  Cost:  0.12407315536886741  Validation cost:  0.18231507923607687\n",
      "Ended iteration 850  Cost:  0.12103451516174464  Validation cost:  0.18153266146611907\n",
      "Ended iteration 900  Cost:  0.11809400105949003  Validation cost:  0.1809218797988415\n",
      "Ended iteration 950  Cost:  0.1154820298901426  Validation cost:  0.18043080352201749\n",
      "======================Third model trained=====================\n",
      "First model validation cost:  0.1879368526309477\n",
      "Second model validation cost:  0.17966524894733238\n",
      "Third model validation cost:  0.18007384149590885\n",
      "Average:  0.18255864769139632\n"
     ]
    }
   ],
   "source": [
    "epochs = 1000\n",
    "model_1hl_sigmoid1 = train_neural_network_1hl(hidden_layer_1_neurons, epochs, fashionTrainParams, fashionTrainTarget, learning_rate, regularization_rate)\n",
    "print(\"======================First model trained=====================\")\n",
    "model_1hl_sigmoid2 = train_neural_network_1hl(hidden_layer_1_neurons, epochs, fashionTrainParams, fashionTrainTarget, learning_rate, regularization_rate)\n",
    "print(\"======================Second model trained=====================\")\n",
    "model_1hl_sigmoid3 = train_neural_network_1hl(hidden_layer_1_neurons, epochs, fashionTrainParams, fashionTrainTarget, learning_rate, regularization_rate)\n",
    "print(\"======================Third model trained=====================\")\n",
    "probs_sigmoid1 = forward_prop_1hl(fashionValidationParams, model_1hl_sigmoid1)\n",
    "cost1 = neuralNetworkCostFunction(probs_sigmoid1, fashionValidationTarget)\n",
    "print(\"First model validation cost: \", cost1)\n",
    "probs_sigmoid2 = forward_prop_1hl(fashionValidationParams, model_1hl_sigmoid2)\n",
    "cost2 = neuralNetworkCostFunction(probs_sigmoid2, fashionValidationTarget)\n",
    "print(\"Second model validation cost: \", cost2)\n",
    "probs_sigmoid3 = forward_prop_1hl(fashionValidationParams, model_1hl_sigmoid3)\n",
    "cost3 = neuralNetworkCostFunction(probs_sigmoid3, fashionValidationTarget)\n",
    "avg = ((cost1+cost2+cost3)/3)\n",
    "print(\"Third model validation cost: \", cost3)\n",
    "print(\"Average: \", avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for 2000 epochs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "epochs=2000\n",
    "model_1hl_sigmoid1 = train_neural_network_1hl(hidden_layer_1_neurons, epochs, fashionTrainParams, fashionTrainTarget, learning_rate, regularization_rate)\n",
    "print(\"======================First model trained=====================\")\n",
    "model_1hl_sigmoid2 = train_neural_network_1hl(hidden_layer_1_neurons, epochs, fashionTrainParams, fashionTrainTarget, learning_rate, regularization_rate)\n",
    "print(\"======================Second model trained=====================\")\n",
    "model_1hl_sigmoid3 = train_neural_network_1hl(hidden_layer_1_neurons, epochs, fashionTrainParams, fashionTrainTarget, learning_rate, regularization_rate)\n",
    "print(\"======================Third model trained=====================\")\n",
    "probs_sigmoid1 = forward_prop_1hl(fashionValidationParams, model_1hl_sigmoid1)\n",
    "cost1 = neuralNetworkCostFunction(probs_sigmoid1, fashionValidationTarget)\n",
    "print(\"First model sigmoid validation cost: \", cost1)\n",
    "probs_sigmoid2 = forward_prop_1hl(fashionValidationParams, model_1hl_sigmoid2)\n",
    "cost2 = neuralNetworkCostFunction(probs_sigmoid2, fashionValidationTarget)\n",
    "print(\"Second model sigmoid validation cost: \", cost2)\n",
    "probs_sigmoid3 = forward_prop_1hl(fashionValidationParams, model_1hl_sigmoid3)\n",
    "cost3 = neuralNetworkCostFunction(probs_sigmoid3, fashionValidationTarget)\n",
    "avg = ((cost1+cost2+cost3)/3)\n",
    "print(\"Third model sigmoid validation cost: \", cost3)\n",
    "print(\"Average: \", avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From these trainings, we see that we achieve best performance generally around 1100 epochs. Now we'll test with other activation functions.\n",
    "\n",
    "## Relu.50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning training...\n",
      "Initialized weights\n",
      "Prepared for mini-batch.\n",
      "Ended iteration 0  Cost:  0.44891177198196647  Validation cost:  0.44764262451701803\n",
      "Ended iteration 50  Cost:  0.2639578082198586  Validation cost:  0.2702709506995608\n",
      "Ended iteration 100  Cost:  0.2006488706812488  Validation cost:  0.2134667499178839\n",
      "Ended iteration 150  Cost:  0.16998019486420005  Validation cost:  0.19116141703346257\n",
      "Ended iteration 200  Cost:  0.1448769425922058  Validation cost:  0.17611739287711717\n",
      "Ended iteration 250  Cost:  0.12369301979864952  Validation cost:  0.16589467566856309\n",
      "Ended iteration 300  Cost:  0.10559218537109814  Validation cost:  0.1592306992279439\n",
      "Ended iteration 350  Cost:  0.09945562296593052  Validation cost:  0.1583065743832179\n",
      "Ended iteration 400  Cost:  0.08788745907715258  Validation cost:  0.15537018960074098\n",
      "Ended iteration 450  Cost:  0.08500968086004791  Validation cost:  0.15944676634797048\n",
      "Ended iteration 500  Cost:  0.07610687815649932  Validation cost:  0.1637212377328492\n",
      "Ended iteration 550  Cost:  0.07001687927010478  Validation cost:  0.16500399872871213\n",
      "Ended iteration 600  Cost:  0.07332558320358297  Validation cost:  0.17040904831495218\n",
      "Ended iteration 650  Cost:  0.05617801128591519  Validation cost:  0.15589240131793167\n",
      "Ended iteration 700  Cost:  0.0536568679682027  Validation cost:  0.16538984140007096\n",
      "Ended iteration 750  Cost:  0.04588924114550832  Validation cost:  0.15610668179895656\n",
      "Ended iteration 800  Cost:  0.04374098705939948  Validation cost:  0.16571396670937827\n",
      "Ended iteration 850  Cost:  0.036370094283055016  Validation cost:  0.1590980726834697\n",
      "Ended iteration 900  Cost:  0.02866636169860451  Validation cost:  0.15642557539125856\n",
      "Ended iteration 950  Cost:  0.032645355808595275  Validation cost:  0.16978006626254322\n",
      "Ended iteration 1000  Cost:  0.026962826755812314  Validation cost:  0.1642307566619124\n",
      "Ended iteration 1050  Cost:  0.016720479999999156  Validation cost:  0.1599053781241072\n",
      "======================First model trained=====================\n",
      "Beginning training...\n",
      "Initialized weights\n",
      "Prepared for mini-batch.\n",
      "Ended iteration 0  Cost:  0.45014114488123785  Validation cost:  0.4489873090771425\n",
      "Ended iteration 50  Cost:  0.26019285527469804  Validation cost:  0.28001751501467714\n",
      "Ended iteration 100  Cost:  0.1837100135905162  Validation cost:  0.21579059279063453\n",
      "Ended iteration 150  Cost:  0.15571553955281217  Validation cost:  0.19529585222359905\n",
      "Ended iteration 200  Cost:  0.1355433033182622  Validation cost:  0.18251525910091884\n",
      "Ended iteration 250  Cost:  0.11809171980464181  Validation cost:  0.1734138065058768\n",
      "Ended iteration 300  Cost:  0.10248859408923483  Validation cost:  0.16699278824255193\n",
      "Ended iteration 350  Cost:  0.08855460505764351  Validation cost:  0.16261628829305233\n",
      "Ended iteration 400  Cost:  0.07618597167212733  Validation cost:  0.16001477928869615\n",
      "Ended iteration 450  Cost:  0.06910358625415637  Validation cost:  0.16183803568097282\n",
      "Ended iteration 500  Cost:  0.06101438290219656  Validation cost:  0.16407207399222418\n",
      "Ended iteration 550  Cost:  0.054500970465895214  Validation cost:  0.16512427099422602\n",
      "Ended iteration 600  Cost:  0.04829733612542959  Validation cost:  0.1659522220659046\n",
      "Ended iteration 650  Cost:  0.042594867952524214  Validation cost:  0.16666065939787894\n",
      "Ended iteration 700  Cost:  0.03718737695780828  Validation cost:  0.16716676337891798\n",
      "Ended iteration 750  Cost:  0.032517563714264244  Validation cost:  0.16777398508986466\n",
      "Ended iteration 800  Cost:  0.02799094472815739  Validation cost:  0.16795910298173725\n",
      "Ended iteration 850  Cost:  0.024123722847595123  Validation cost:  0.16824044425274184\n",
      "Ended iteration 900  Cost:  0.02062211993410881  Validation cost:  0.16834067294714739\n",
      "Ended iteration 950  Cost:  0.017754229057232854  Validation cost:  0.16867392547011753\n",
      "Ended iteration 1000  Cost:  0.015224875791933122  Validation cost:  0.16894383220130224\n",
      "Ended iteration 1050  Cost:  0.012992379486221253  Validation cost:  0.16915528290400905\n",
      "======================Second model trained=====================\n",
      "Beginning training...\n",
      "Initialized weights\n",
      "Prepared for mini-batch.\n",
      "Ended iteration 0  Cost:  0.4505147033805503  Validation cost:  0.44835137790299767\n",
      "Ended iteration 50  Cost:  0.26213927026306794  Validation cost:  0.27001222076219383\n",
      "Ended iteration 100  Cost:  0.1895953020301532  Validation cost:  0.20907812087624272\n",
      "Ended iteration 150  Cost:  0.15586840683649775  Validation cost:  0.1860590133708048\n",
      "Ended iteration 200  Cost:  0.13044007967968688  Validation cost:  0.17116210137052526\n",
      "Ended iteration 250  Cost:  0.11104026488002883  Validation cost:  0.16164798236849523\n",
      "Ended iteration 300  Cost:  0.09571014864490428  Validation cost:  0.15580660467707633\n",
      "Ended iteration 350  Cost:  0.08279044955367068  Validation cost:  0.15221198416233273\n",
      "Ended iteration 400  Cost:  0.07147848403402539  Validation cost:  0.15001978240390898\n",
      "Ended iteration 450  Cost:  0.061618086906920326  Validation cost:  0.14876505480873536\n",
      "Ended iteration 500  Cost:  0.05311684181564524  Validation cost:  0.14821030311704295\n",
      "Ended iteration 550  Cost:  0.05714980861788796  Validation cost:  0.1577602232075213\n",
      "Ended iteration 600  Cost:  0.04128403370378982  Validation cost:  0.14857332248421506\n",
      "Ended iteration 650  Cost:  0.03593348093746834  Validation cost:  0.14878526962943717\n",
      "Ended iteration 700  Cost:  0.031433658234844966  Validation cost:  0.14944160495794612\n",
      "Ended iteration 750  Cost:  0.02755580362546284  Validation cost:  0.15026355697435204\n",
      "Ended iteration 800  Cost:  0.024196650948953256  Validation cost:  0.15117216694805682\n",
      "Ended iteration 850  Cost:  0.02121112558094403  Validation cost:  0.15222332296386104\n",
      "Ended iteration 900  Cost:  0.02381979437606069  Validation cost:  0.15916393745011778\n",
      "Ended iteration 950  Cost:  0.01725684847435842  Validation cost:  0.15515407965237765\n",
      "Ended iteration 1000  Cost:  0.014470941191302007  Validation cost:  0.15511700690882826\n",
      "Ended iteration 1050  Cost:  0.012535737175723718  Validation cost:  0.15575086929793716\n",
      "======================Third model trained=====================\n",
      "First model relu validation cost:  0.1609687862386679\n",
      "Second model relu validation cost:  0.16789629625002564\n",
      "Third model relu validation cost:  0.15650911428316197\n",
      "Average:  0.1617913989239518\n"
     ]
    }
   ],
   "source": [
    "epochs = 1100\n",
    "model_1hl_relu1 = train_neural_network_1hl(hidden_layer_1_neurons, epochs, fashionTrainParams, fashionTrainTarget, learning_rate, regularization_rate, activation=\"relu\")\n",
    "print(\"======================First model trained=====================\")\n",
    "model_1hl_relu2 = train_neural_network_1hl(hidden_layer_1_neurons, epochs, fashionTrainParams, fashionTrainTarget, learning_rate, regularization_rate, activation=\"relu\")\n",
    "print(\"======================Second model trained=====================\")\n",
    "model_1hl_relu3 = train_neural_network_1hl(hidden_layer_1_neurons, epochs, fashionTrainParams, fashionTrainTarget, learning_rate, regularization_rate, activation=\"relu\")\n",
    "print(\"======================Third model trained=====================\")\n",
    "probs_sigmoid1 = forward_prop_1hl(fashionValidationParams, model_1hl_sigmoid1)\n",
    "cost1 = neuralNetworkCostFunction(probs_sigmoid1, fashionValidationTarget)\n",
    "print(\"First model relu validation cost: \", cost1)\n",
    "probs_sigmoid2 = forward_prop_1hl(fashionValidationParams, model_1hl_sigmoid2)\n",
    "cost2 = neuralNetworkCostFunction(probs_sigmoid2, fashionValidationTarget)\n",
    "print(\"Second model relu validation cost: \", cost2)\n",
    "probs_sigmoid3 = forward_prop_1hl(fashionValidationParams, model_1hl_sigmoid3)\n",
    "cost3 = neuralNetworkCostFunction(probs_sigmoid3, fashionValidationTarget)\n",
    "avg = ((cost1+cost2+cost3)/3)\n",
    "print(\"Third model relu validation cost: \", cost3)\n",
    "print(\"Average: \", avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leaky-relu.50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "epochs = 2000\n",
    "model_1hl_leaky_relu1 = train_neural_network_1hl(hidden_layer_1_neurons, epochs, fashionTrainParams, fashionTrainTarget, learning_rate, regularization_rate, activation=\"leaky-relu\")\n",
    "print(\"======================First model trained=====================\")\n",
    "model_1hl_leaky_relu2 = train_neural_network_1hl(hidden_layer_1_neurons, epochs, fashionTrainParams, fashionTrainTarget, learning_rate, regularization_rate, activation=\"leaky-relu\")\n",
    "print(\"======================Second model trained=====================\")\n",
    "model_1hl_leaky_relu3 = train_neural_network_1hl(hidden_layer_1_neurons, epochs, fashionTrainParams, fashionTrainTarget, learning_rate, regularization_rate, activation=\"leaky-relu\")\n",
    "print(\"======================Third model trained=====================\")\n",
    "probs_sigmoid1 = forward_prop_1hl(fashionValidationParams, model_1hl_sigmoid1)\n",
    "cost1 = neuralNetworkCostFunction(probs_sigmoid1, fashionValidationTarget)\n",
    "print(\"First model leaky-relu validation cost: \", cost1)\n",
    "probs_sigmoid2 = forward_prop_1hl(fashionValidationParams, model_1hl_sigmoid2)\n",
    "cost2 = neuralNetworkCostFunction(probs_sigmoid2, fashionValidationTarget)\n",
    "print(\"Second model leaky-relu validation cost: \", cost2)\n",
    "probs_sigmoid3 = forward_prop_1hl(fashionValidationParams, model_1hl_sigmoid3)\n",
    "cost3 = neuralNetworkCostFunction(probs_sigmoid3, fashionValidationTarget)\n",
    "avg = ((cost1+cost2+cost3)/3)\n",
    "print(\"Third model leaky-relu validation cost: \", cost3)\n",
    "print(\"Average: \", avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Changing hidden layer size\n",
    "Now we will experiment changing the amount of neurons on the hidden layer and see the impact on different activation functions. We will fix the amount of epochs to 1350, which generates an average-to-good result for all activation functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_layer_1_neurons = 100\n",
    "epochs = 1350"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sigmoid.100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_1hl_sigmoid1 = train_neural_network_1hl(hidden_layer_1_neurons, epochs, fashionTrainParams, fashionTrainTarget, learning_rate, regularization_rate)\n",
    "print(\"======================First model trained=====================\")\n",
    "model_1hl_sigmoid2 = train_neural_network_1hl(hidden_layer_1_neurons, epochs, fashionTrainParams, fashionTrainTarget, learning_rate, regularization_rate)\n",
    "print(\"======================Second model trained=====================\")\n",
    "model_1hl_sigmoid3 = train_neural_network_1hl(hidden_layer_1_neurons, epochs, fashionTrainParams, fashionTrainTarget, learning_rate, regularization_rate)\n",
    "print(\"======================Third model trained=====================\")\n",
    "probs_sigmoid1 = forward_prop_1hl(fashionValidationParams, model_1hl_sigmoid1)\n",
    "cost1 = neuralNetworkCostFunction(probs_sigmoid1, fashionValidationTarget)\n",
    "print(\"First model sigmoid validation cost: \", cost1)\n",
    "probs_sigmoid2 = forward_prop_1hl(fashionValidationParams, model_1hl_sigmoid2)\n",
    "cost2 = neuralNetworkCostFunction(probs_sigmoid2, fashionValidationTarget)\n",
    "print(\"Second model sigmoid validation cost: \", cost2)\n",
    "probs_sigmoid3 = forward_prop_1hl(fashionValidationParams, model_1hl_sigmoid3)\n",
    "cost3 = neuralNetworkCostFunction(probs_sigmoid3, fashionValidationTarget)\n",
    "avg = ((cost1+cost2+cost3)/3)\n",
    "print(\"Third model sigmoid validation cost: \", cost3)\n",
    "print(\"Average: \", avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relu.100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1hl_relu1 = train_neural_network_1hl(hidden_layer_1_neurons, epochs, fashionTrainParams, fashionTrainTarget, learning_rate, regularization_rate, activation=\"relu\")\n",
    "print(\"======================First model trained=====================\")\n",
    "model_1hl_relu2 = train_neural_network_1hl(hidden_layer_1_neurons, epochs, fashionTrainParams, fashionTrainTarget, learning_rate, regularization_rate, activation=\"relu\")\n",
    "print(\"======================Second model trained=====================\")\n",
    "model_1hl_relu3 = train_neural_network_1hl(hidden_layer_1_neurons, epochs, fashionTrainParams, fashionTrainTarget, learning_rate, regularization_rate, activation=\"relu\")\n",
    "print(\"======================Third model trained=====================\")\n",
    "probs_sigmoid1 = forward_prop_1hl(fashionValidationParams, model_1hl_sigmoid1)\n",
    "cost1 = neuralNetworkCostFunction(probs_sigmoid1, fashionValidationTarget)\n",
    "print(\"First model relu validation cost: \", cost1)\n",
    "probs_sigmoid2 = forward_prop_1hl(fashionValidationParams, model_1hl_sigmoid2)\n",
    "cost2 = neuralNetworkCostFunction(probs_sigmoid2, fashionValidationTarget)\n",
    "print(\"Second model relu validation cost: \", cost2)\n",
    "probs_sigmoid3 = forward_prop_1hl(fashionValidationParams, model_1hl_sigmoid3)\n",
    "cost3 = neuralNetworkCostFunction(probs_sigmoid3, fashionValidationTarget)\n",
    "avg = ((cost1+cost2+cost3)/3)\n",
    "print(\"Third model relu validation cost: \", cost3)\n",
    "print(\"Average: \", avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leaky Relu.100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1hl_leaky_relu1 = train_neural_network_1hl(hidden_layer_1_neurons, epochs, fashionTrainParams, fashionTrainTarget, learning_rate, regularization_rate, activation=\"leaky-relu\")\n",
    "print(\"======================First model trained=====================\")\n",
    "model_1hl_leaky_relu2 = train_neural_network_1hl(hidden_layer_1_neurons, epochs, fashionTrainParams, fashionTrainTarget, learning_rate, regularization_rate, activation=\"leaky-relu\")\n",
    "print(\"======================Second model trained=====================\")\n",
    "model_1hl_leaky_relu3 = train_neural_network_1hl(hidden_layer_1_neurons, epochs, fashionTrainParams, fashionTrainTarget, learning_rate, regularization_rate, activation=\"leaky-relu\")\n",
    "print(\"======================Third model trained=====================\")\n",
    "probs_sigmoid1 = forward_prop_1hl(fashionValidationParams, model_1hl_sigmoid1)\n",
    "cost1 = neuralNetworkCostFunction(probs_sigmoid1, fashionValidationTarget)\n",
    "print(\"First model leaky-relu validation cost: \", cost1)\n",
    "probs_sigmoid2 = forward_prop_1hl(fashionValidationParams, model_1hl_sigmoid2)\n",
    "cost2 = neuralNetworkCostFunction(probs_sigmoid2, fashionValidationTarget)\n",
    "print(\"Second model leaky-relu validation cost: \", cost2)\n",
    "probs_sigmoid3 = forward_prop_1hl(fashionValidationParams, model_1hl_sigmoid3)\n",
    "cost3 = neuralNetworkCostFunction(probs_sigmoid3, fashionValidationTarget)\n",
    "avg = ((cost1+cost2+cost3)/3)\n",
    "print(\"Third model leaky-relu validation cost: \", cost3)\n",
    "print(\"Average: \", avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expanding to 500 neurons on the hidden layer..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_layer_1_neurons = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sigmoid.500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1hl_sigmoid1 = train_neural_network_1hl(hidden_layer_1_neurons, epochs, fashionTrainParams, fashionTrainTarget, learning_rate, regularization_rate)\n",
    "print(\"======================First model trained=====================\")\n",
    "model_1hl_sigmoid2 = train_neural_network_1hl(hidden_layer_1_neurons, epochs, fashionTrainParams, fashionTrainTarget, learning_rate, regularization_rate)\n",
    "print(\"======================Second model trained=====================\")\n",
    "model_1hl_sigmoid3 = train_neural_network_1hl(hidden_layer_1_neurons, epochs, fashionTrainParams, fashionTrainTarget, learning_rate, regularization_rate)\n",
    "print(\"======================Third model trained=====================\")\n",
    "probs_sigmoid1 = forward_prop_1hl(fashionValidationParams, model_1hl_sigmoid1)\n",
    "cost1 = neuralNetworkCostFunction(probs_sigmoid1, fashionValidationTarget)\n",
    "print(\"First model sigmoid validation cost: \", cost1)\n",
    "probs_sigmoid2 = forward_prop_1hl(fashionValidationParams, model_1hl_sigmoid2)\n",
    "cost2 = neuralNetworkCostFunction(probs_sigmoid2, fashionValidationTarget)\n",
    "print(\"Second model sigmoid validation cost: \", cost2)\n",
    "probs_sigmoid3 = forward_prop_1hl(fashionValidationParams, model_1hl_sigmoid3)\n",
    "cost3 = neuralNetworkCostFunction(probs_sigmoid3, fashionValidationTarget)\n",
    "avg = ((cost1+cost2+cost3)/3)\n",
    "print(\"Third model sigmoid validation cost: \", cost3)\n",
    "print(\"Average: \", avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relu.500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1hl_relu1 = train_neural_network_1hl(hidden_layer_1_neurons, epochs, fashionTrainParams, fashionTrainTarget, learning_rate, regularization_rate, activation=\"relu\")\n",
    "print(\"======================First model trained=====================\")\n",
    "model_1hl_relu2 = train_neural_network_1hl(hidden_layer_1_neurons, epochs, fashionTrainParams, fashionTrainTarget, learning_rate, regularization_rate, activation=\"relu\")\n",
    "print(\"======================Second model trained=====================\")\n",
    "model_1hl_relu3 = train_neural_network_1hl(hidden_layer_1_neurons, epochs, fashionTrainParams, fashionTrainTarget, learning_rate, regularization_rate, activation=\"relu\")\n",
    "print(\"======================Third model trained=====================\")\n",
    "probs_sigmoid1 = forward_prop_1hl(fashionValidationParams, model_1hl_sigmoid1)\n",
    "cost1 = neuralNetworkCostFunction(probs_sigmoid1, fashionValidationTarget)\n",
    "print(\"First model relu validation cost: \", cost1)\n",
    "probs_sigmoid2 = forward_prop_1hl(fashionValidationParams, model_1hl_sigmoid2)\n",
    "cost2 = neuralNetworkCostFunction(probs_sigmoid2, fashionValidationTarget)\n",
    "print(\"Second model relu validation cost: \", cost2)\n",
    "probs_sigmoid3 = forward_prop_1hl(fashionValidationParams, model_1hl_sigmoid3)\n",
    "cost3 = neuralNetworkCostFunction(probs_sigmoid3, fashionValidationTarget)\n",
    "avg = ((cost1+cost2+cost3)/3)\n",
    "print(\"Third model relu validation cost: \", cost3)\n",
    "print(\"Average: \", avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leaky Relu.500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1hl_leaky_relu1 = train_neural_network_1hl(hidden_layer_1_neurons, epochs, fashionTrainParams, fashionTrainTarget, learning_rate, regularization_rate, activation=\"leaky-relu\")\n",
    "print(\"======================First model trained=====================\")\n",
    "model_1hl_leaky_relu2 = train_neural_network_1hl(hidden_layer_1_neurons, epochs, fashionTrainParams, fashionTrainTarget, learning_rate, regularization_rate, activation=\"leaky-relu\")\n",
    "print(\"======================Second model trained=====================\")\n",
    "model_1hl_leaky_relu3 = train_neural_network_1hl(hidden_layer_1_neurons, epochs, fashionTrainParams, fashionTrainTarget, learning_rate, regularization_rate, activation=\"leaky-relu\")\n",
    "print(\"======================Third model trained=====================\")\n",
    "probs_sigmoid1 = forward_prop_1hl(fashionValidationParams, model_1hl_sigmoid1)\n",
    "cost1 = neuralNetworkCostFunction(probs_sigmoid1, fashionValidationTarget)\n",
    "print(\"First model leaky-relu validation cost: \", cost1)\n",
    "probs_sigmoid2 = forward_prop_1hl(fashionValidationParams, model_1hl_sigmoid2)\n",
    "cost2 = neuralNetworkCostFunction(probs_sigmoid2, fashionValidationTarget)\n",
    "print(\"Second model leaky-relu validation cost: \", cost2)\n",
    "probs_sigmoid3 = forward_prop_1hl(fashionValidationParams, model_1hl_sigmoid3)\n",
    "cost3 = neuralNetworkCostFunction(probs_sigmoid3, fashionValidationTarget)\n",
    "avg = ((cost1+cost2+cost3)/3)\n",
    "print(\"Third model leaky-relu validation cost: \", cost3)\n",
    "print(\"Average: \", avg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
