{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "Here, we'll explore neural networks applied to the fashion-mnist problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import log_loss\n",
    "import csv\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions\n",
    "\n",
    "def formatArray (dataFrame, columnToExtract) :\n",
    "    array = dataFrame.values\n",
    "    target = array[:,columnToExtract]\n",
    "    params = np.delete(array, columnToExtract, axis = 1)\n",
    "    return params, target\n",
    "\n",
    "def loadFashionTrainData():\n",
    "    return pd.read_csv(\"fashion-mnist-dataset/fashion-mnist_train.csv\")\n",
    "\n",
    "def loadFashionTestData():\n",
    "    return pd.read_csv(\"fashion-mnist-dataset/fashion-mnist_test.csv\")\n",
    "\n",
    "def split_train_test(data, test_ratio):\n",
    "    np.random.seed(42)\n",
    "    shuffled_indices = np.random.permutation(len(data))\n",
    "    test_set_size = int(len(data) * test_ratio)\n",
    "    test_indices = shuffled_indices[:test_set_size]\n",
    "    train_indices = shuffled_indices[test_set_size:]\n",
    "    return data.iloc[train_indices], data.iloc[test_indices]\n",
    "\n",
    "def createTarget (target):\n",
    "    results = np.zeros((target.size, 10), dtype=int)\n",
    "    for i in range(10):\n",
    "        for j in range(target.size):\n",
    "            if (target[j] != i):\n",
    "                results[j][i - 1] = 0\n",
    "            else:\n",
    "                results[j][i - 1] = 1\n",
    "    return results\n",
    "\n",
    "def p_print(a):\n",
    "    for x in a:\n",
    "        print(*x, sep=\" \")\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The dataset\n",
    "First and foremost, we'll open train and test data. The training data is split to obtain validation items and the the target values are also separated from the original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "fashionTrainDataset = loadFashionTrainData()\n",
    "fashionTestDataset = loadFashionTestData()\n",
    "fashionTrain, fashionValidation = split_train_test(fashionTrainDataset, 0.2)\n",
    "fashionTrainParams, fashionTrainTarget = formatArray(fashionTrain, 0)\n",
    "fashionTrainTarget = createTarget(fashionTrainTarget)\n",
    "fashionTrainParams = fashionTrainParams/255\n",
    "fashionValidationParams, fashionValidationTarget = formatArray(fashionValidation, 0)\n",
    "fashionValidationTarget = createTarget(fashionValidationTarget)\n",
    "fashionValidationParams = fashionValidationParams/255\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation and Softmax Functions\n",
    "First, we'll start by implementing some useful functions seen in class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(n):\n",
    "    return (1/(1+np.exp(-n)))\n",
    "\n",
    "def derivative_sigmoid(n):\n",
    "    x = sigmoid(n)\n",
    "    return x * (1 - x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(n):\n",
    "    if n < 0:\n",
    "        return 0\n",
    "    return n\n",
    "\n",
    "def derivative_relu(n):\n",
    "    if n < 0:\n",
    "        return 0\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leaky_relu(n):\n",
    "    if n > 0:\n",
    "        return n\n",
    "    return 0.01 * n\n",
    "\n",
    "def derivative_leaky_relu(n):\n",
    "    if n < 0:\n",
    "        return 0.01\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(n):\n",
    "    exp = np.exp(n)\n",
    "    test_sum = np.sum(exp, axis=1, keepdims=True)\n",
    "    return exp/test_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Propagation\n",
    "In this section, we define forward propagation related functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_1hl(input_dimension,hidden_layer_1_neurons, output_dimension):\n",
    "    neural_data = {}\n",
    "    np.random.seed()\n",
    "    neural_data['w1'] = np.random.randn(input_dimension, hidden_layer_1_neurons)/ np.sqrt(input_dimension)\n",
    "    neural_data['w2'] = np.random.randn(hidden_layer_1_neurons, output_dimension)/ np.sqrt(input_dimension)\n",
    "    \n",
    "    neural_data['b1'] = np.zeros((1, hidden_layer_1_neurons))\n",
    "    neural_data['b2'] = np.zeros((1, output_dimension))\n",
    "    return neural_data\n",
    "\n",
    "def initialize_2hl(input_dimension, hidden_layer_1_neurons, hidden_layer_2_neurons, output_dimension):\n",
    "    neural_data = {}\n",
    "    np.random.seed(0)\n",
    "\n",
    "    neural_data['w1'] = np.random.randn(input_dimension, hidden_layer_1_neurons)/ np.sqrt(input_dimension)\n",
    "    neural_data['w2'] = np.random.randn(hidden_layer_1_neurons, hidden_layer_2_neurons)/ np.sqrt(hidden_layer_1_neurons)\n",
    "    neural_data['w3'] = np.random.randn(hidden_layer_2_neurons, output_dimension)/ np.sqrt(hidden_layer_2_neurons)\n",
    "\n",
    "    neural_data['b1'] = np.zeros((1, hidden_layer_1_neurons))\n",
    "    neural_data['b2'] = np.zeros((1, hidden_layer_2_neurons))\n",
    "    neural_data['b3'] = np.zeros((1, output_dimension))\n",
    "    return neural_data\n",
    "    \n",
    "def forward_prop_1hl(x, neural_data, activation=\"sigmoid\"):\n",
    "    w1 , w2, b1, b2 = neural_data['w1'], neural_data['w2'], neural_data['b1'], neural_data['b2']\n",
    "    x1 = np.dot(x, w1) + b1 #Output of hidden layer\n",
    "    if activation == \"sigmoid\":\n",
    "        y1 = np.asarray([[sigmoid(n) for n in j] for j in x1]) #Output of hidden layer with activation function\n",
    "    elif activation == \"relu\":\n",
    "        y1 = np.asarray([[relu(n) for n in j] for j in x1]) #Output of hidden layer with activation function\n",
    "    elif activation == \"leaky-relu\":\n",
    "        y1 = np.asarray([[leaky_relu(n) for n in j] for j in x1]) #Output of hidden layer with activation function\n",
    "    x2 = np.dot(y1, w2) + b2 #Output of last layer\n",
    "    neural_data['x1'] = x1\n",
    "    neural_data['x2'] = x2\n",
    "    neural_data['y1'] = y1\n",
    "    neural_data['o'] = softmax(x2)  # Final output with softmax\n",
    "\n",
    "    return neural_data['o']\n",
    "\n",
    "def forward_prop_2hl(x, neural_data):\n",
    "    w1 , w2, w3, b1, b2, b3 = neural_data['w1'], neural_data['w2'], neural_data['w3'], neural_data['b1'], neural_data['b2'], neural_data['b3']\n",
    "\n",
    "    x1 = np.dot(x, w1) + b1\n",
    "    y1 = np.asarray([[relu(n) for n in j] for j in x1])\n",
    "    x2 = np.dot(a1, w2) + b2\n",
    "    y2 = np.asarray([[relu(n) for n in j] for j in x2])\n",
    "    x3 = np.dot(a2, w3) + b3\n",
    "    \n",
    "    neural_data['x1'] = x1\n",
    "    neural_data['x2'] = x2\n",
    "    neural_data['x3'] = x3\n",
    "\n",
    "    neural_data['y1'] = y1\n",
    "    neural_data['y2'] = y2\n",
    "    \n",
    "    neural_data['o'] = softmax(x3)\n",
    "    return neural_data['o']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction Functions\n",
    "Helper functions that return predictions, given our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_1hl(x, neural_data):\n",
    "    test = forward_prop_1hl(x,neural_data)\n",
    "    return np.argmax(test, axis=1)\n",
    "\n",
    "def predict_2hl(x, neural_data):\n",
    "    return np.argmax(forward_prop_2hl(x,neural_data), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neuralNetworkCostFunction(output, target):\n",
    "#     fashionTargetMinusOne = fashionTarget - 1\n",
    "#     cost = 0\n",
    "#     for j in range(fashionTrainOutput.shape[0]):\n",
    "#         cost += np.sum(np.multiply(fashionTarget, np.log10(fashionTrainOutput[j])),np.multiply(fashionTargetMinusOne, (1- np.log10(fashionTrainOutput[j]))))\n",
    "#     cost = cost*(-1)/fashionTrainOutput.shape[0]\n",
    "    \n",
    "    \n",
    "    \n",
    "#     partial_cost = -np.log(probs[range(fashionTrainOutput.shape[0]), fashionTarget])\n",
    "#     partial_cost -= np.log(1-probs[range(fashionTrainOutput.shape[0]), fashionTarget])\n",
    "#     cost = np.sum(partial_cost)\n",
    "    \n",
    "    cost = log_loss(target, output)\n",
    "    return cost\n",
    "\n",
    "def meanSquaresCost(fashionTrainOutput, fashionTarget):\n",
    "    diference = fashionTrainOutput - fashionTarget\n",
    "    squareDiference = diference ** 2\n",
    "    n = fashionTrainOutput.shape[0]    \n",
    "    return (np.sum(squareDiference)/(2*n))\n",
    "\n",
    "def accuracy (target, params, neural_model):\n",
    "    right_answers = 0\n",
    "    target_indexes = np.argmax(target, axis=1)\n",
    "    predicted = predict(params, neural_model)\n",
    "    n = neural_model.shape[0]\n",
    "    for i in range (n):\n",
    "        if (target_indexes[i] == predicted[i]):\n",
    "            right_answers = right_answers + 1\n",
    "    return right_answers/n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and backward propagation\n",
    "\n",
    "Some notes:\n",
    "\n",
    "### For one hidden layer:\n",
    "- x1 = #Output of hidden layer\n",
    "- x2 = #Output of last layer\n",
    "- y1 = #Output of hidden layer with activation function\n",
    "- o = Final output with Softmax\n",
    "\n",
    "### For TWO hidden layers:\n",
    "- x1 = #Output of first hidden layer\n",
    "- x2 = #Output of second hidden layer\n",
    "- x3 = Output of last layer\n",
    "- y1 = #Output of first hidden layer with activation function\n",
    "- y2 = #Output of second hidden layer with activation function\n",
    "- o = Final output with Softmax\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One hidden layer\n",
    "\n",
    "Here, we present our code and results achieved by a learning algorithm that uses a neural network with only one hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_neural_network_1hl(hidden_layer_1_neurons, epochs, trainParams, trainTarget, learning_rate, regularization_rate, activation=\"sigmoid\"):\n",
    "    print(\"Beginning training with \", epochs, \" epochs and \", hidden_layer_1_neurons, \" hidden neurons.\")\n",
    "    input_dimension = 784\n",
    "    output_dimension = 10\n",
    "#     Initializes weights and biases for our neural network\n",
    "    neural_data = initialize_1hl(input_dimension, hidden_layer_1_neurons, output_dimension)\n",
    "    print(\"Initialized weights\")\n",
    "    sigmoid = True\n",
    "    relu = False\n",
    "    leaky_relu = False\n",
    "    \n",
    "#     Prepares for mini-batch\n",
    "    batchSize = 600\n",
    "    start_idx = 0\n",
    "    indices = np.arange(trainParams.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "    print(\"Prepared for mini-batch.\")\n",
    "#     Performs Backpropagation\n",
    "    capitalDelta3 = 0\n",
    "    capitalDelta2 = 0\n",
    "    for j in range(epochs):\n",
    "        excerpt = indices[start_idx:start_idx + batchSize]\n",
    "        mini_batch_data = trainParams[excerpt]\n",
    "        miniBatchTarget = trainTarget[excerpt]\n",
    "#         Performs Forward propagation\n",
    "        probs = forward_prop_1hl(mini_batch_data, neural_data, activation)\n",
    "    \n",
    "\n",
    "#         Performs Backward propagation\n",
    "\n",
    "        delta3 = probs - miniBatchTarget\n",
    "\n",
    "        dW2 =(1./batchSize)* (neural_data['y1'].T).dot(delta3)\n",
    "        db2 =(1./batchSize)* ( np.sum(delta3, axis=0, keepdims=True))\n",
    "        delta2 = np.dot(delta3, neural_data['w2'].T)\n",
    "        aux = neural_data['y1']\n",
    "        if activation == \"sigmoid\":\n",
    "            aux = [[derivative_sigmoid(n) for n in x] for x in aux]\n",
    "        if activation == \"relu\":\n",
    "            aux = [[derivative_relu(n) for n in x] for x in aux]\n",
    "        if activation == \"leaky_relu\":\n",
    "            aux = [[derivative_leaky_relu(n) for n in x] for x in aux]\n",
    "\n",
    "        delta2 = delta2 * aux\n",
    "\n",
    "        dW1 = (1./batchSize)*np.dot(mini_batch_data.T, delta2)\n",
    "        db1 = (1./batchSize)*np.sum(delta2, axis=0)\n",
    "        \n",
    "        \n",
    "#          # Performs regularization\n",
    "#         dW2 += regularization_rate * neural_data['w2']\n",
    "#         dW1 += regularization_rate * neural_data['w1']\n",
    "\n",
    "        # Gradient descent parameter update\n",
    "        neural_data['w1'] -= learning_rate * dW1\n",
    "        neural_data['b1'] -= learning_rate * db1\n",
    "        neural_data['w2'] -= learning_rate * dW2\n",
    "        neural_data['b2'] -= learning_rate * db2\n",
    "        \n",
    "        if j%50 == 0:\n",
    "            #         Calculates costs\n",
    "\n",
    "            cost = neuralNetworkCostFunction(probs, miniBatchTarget)\n",
    "            validation_probs = forward_prop_1hl(fashionValidationParams, neural_data)\n",
    "            validation_cost = neuralNetworkCostFunction(validation_probs, fashionValidationTarget)\n",
    "            print(\"Ended iteration\", j,\" Cost: \", cost, \" Validation cost: \", validation_cost)\n",
    "        start_idx += batchSize;\n",
    "        start_idx %= mini_batch_data.shape[0]\n",
    "    return neural_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Two hidden layers\n",
    "\n",
    "Same as before, but for 2 hidden layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_neural_network_2hl(hidden_layer_1_neurons, hidden_layer_2_neurons, epochs, trainParams, trainTarget, learning_rate, regularization_rate):\n",
    "    input_dimension = 784\n",
    "    output_dimension = 10\n",
    "#     Initializes weights and biases for our neural network\n",
    "    neural_data = initialize_2hl(input_dimension, hidden_layer_1_neurons, hidden_layer_2_neurons, output_dimension)\n",
    "    sigmoid = True\n",
    "    relu = False\n",
    "    leaky_relu = False\n",
    "    \n",
    "#     Prepares for mini-batch\n",
    "    batchSize = 64\n",
    "    start_idx = 0;\n",
    "    indices = np.arange(trainParams.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "    excerpt = indices[start_idx:start_idx + batchSize]\n",
    "    mini_batch_data = trainParams[excerpt]\n",
    "    miniBatchTarget = createTarget(trainTarget[excerpt])\n",
    "#     Performs Backpropagation\n",
    "    for j in range(epochs):\n",
    "\n",
    "#         Performs Forward propagation\n",
    "        probs = forward_prop_1hl(mini_batch_data, neural_data)\n",
    "        \n",
    "#         Calculates cost\n",
    "\n",
    "#         Performs Backward propagation\n",
    "        delta4 = probs - miniBatchTarget\n",
    "        dW3 = (neural_data['y2'].T).dot(delta4)\n",
    "        db3 = np.sum(delta4, axis=0, keepdims=True)\n",
    "        delta3 = delta4.dot(neural_data['w3'].T)\n",
    "        aux = neural_data['y2']\n",
    "        if sigmoid:\n",
    "            aux = [[derivative_sigmoid(n) for n in x] for x in aux]\n",
    "        if relu:\n",
    "            aux = [[derivative_relu(n) for n in x] for x in aux]\n",
    "        if leaky_relu:\n",
    "            aux = [[derivative_leaky_relu(n) for n in x] for x in aux]        \n",
    "        delta3 = delta3 * aux\n",
    "        dW2 = np.dot(mini_batch_data.T, delta3)\n",
    "        db2 = np.sum(delta3, axis=0)\n",
    "        delta2 = delta3.dot(neural_data['w2'].T)  #look for issues here\n",
    "        aux = neural_data['y1']\n",
    "        if sigmoid:\n",
    "            aux = [[derivative_sigmoid(n) for n in x] for x in aux]\n",
    "        if relu:\n",
    "            aux = [[derivative_relu(n) for n in x] for x in aux]\n",
    "        if leaky_relu:\n",
    "            aux = [[derivative_leaky_relu(n) for n in x] for x in aux]\n",
    "        delta2 = delta2 * aux\n",
    "        dW1 = np.dot(mini_batch_data.T, delta2)\n",
    "        db1 = np.sum(delta2, axis=0)      \n",
    "        \n",
    "#          # Performs regularization\n",
    "#         dW3 += regularization_rate * neural_data['w3']\n",
    "#         dW2 += regularization_rate * neural_data['w2']\n",
    "#         dW1 += regularization_rate * neural_data['w1']\n",
    "\n",
    "        # Gradient descent parameter update\n",
    "        \n",
    "        neural_data['w1'] += -learning_rate * dW1\n",
    "        neural_data['b1'] += -learning_rate * db1\n",
    "        neural_data['w2'] += -learning_rate * dW2\n",
    "        neural_data['b2'] += -learning_rate * db2\n",
    "        neural_data['w3'] += -learning_rate * dW3\n",
    "        neural_data['b3'] += -learning_rate * db3\n",
    "        \n",
    "        print(\"Ended iteration\", j)\n",
    "        start_idx += 1;\n",
    "        start_idx %= mini_batch_data.shape[0]\n",
    "        \n",
    "    return neural_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the neural networks\n",
    "\n",
    "Now, we'll test our neural networks under multiple circumstances on the validation set, so we can gest the best possible models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem details\n",
    "- Input dimension: 28x28 = 784 neurons\n",
    "- Output dimension: 10 classes = 10 neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_layers = 1\n",
    "input_neurons = 784\n",
    "output_neurons = 10\n",
    "hidden_layer_1_neurons = 50\n",
    "hidden_layer_2_neurons = 15\n",
    "learning_rate = 0.1\n",
    "regularization_rate = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First experimentations\n",
    "First, we will train a model using a hidden layer with 50 neurons and 1000 epochs, which is small given the input of 784 neurons. We will run 3 times for each activation function to get an average result (that depends heavily on the initialization of the weights)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_neural_network_1hl(hidden_layer_1_neurons, epochs, fashionTrainParams, fashionTrainTarget, learning_rate, regularization_rate, activation=\"sigmoid\"):\n",
    "    model_1hl_1 = train_neural_network_1hl(hidden_layer_1_neurons, epochs, fashionTrainParams, fashionTrainTarget, learning_rate, regularization_rate, activation)\n",
    "    print(\"======================First model trained=====================\")\n",
    "    model_1hl_2 = train_neural_network_1hl(hidden_layer_1_neurons, epochs, fashionTrainParams, fashionTrainTarget, learning_rate, regularization_rate, activation)\n",
    "    print(\"======================Second model trained=====================\")\n",
    "    model_1hl_3 = train_neural_network_1hl(hidden_layer_1_neurons, epochs, fashionTrainParams, fashionTrainTarget, learning_rate, regularization_rate, activation)\n",
    "    print(\"======================Third model trained=====================\")\n",
    "    probs_sigmoid1 = forward_prop_1hl(fashionValidationParams, model_1hl_sigmoid1)\n",
    "    cost1 = neuralNetworkCostFunction(probs_sigmoid1, fashionValidationTarget)\n",
    "    acc1_train = accuracy(fashionTrainTarget, fashionTrainParams, model_1hl_1)\n",
    "    acc1_validation = accuracy(fashionValidationTarget, fashionValidationParams, model_1hl_1)\n",
    "    print(\"First model \", activation, \"validation cost: \", cost1, \" acc_train: \", acc1_train, \" acc_validation: \", acc1_validation)\n",
    "    \n",
    "    probs_sigmoid2 = forward_prop_1hl(fashionValidationParams, model_1hl_sigmoid2)\n",
    "    cost2 = neuralNetworkCostFunction(probs_sigmoid2, fashionValidationTarget)\n",
    "    acc2_train = accuracy(fashionTrainTarget, fashionTrainParams, model_1hl_2)\n",
    "    acc2_validation = accuracy(fashionValidationTarget, fashionValidationParams, model_1hl_2)\n",
    "    print(\"Second model  \", activation, \"validation cost: \", cost2, \" acc_train: \", acc2_train, \" acc_validation: \", acc2_validation)\n",
    "    \n",
    "    probs_sigmoid3 = forward_prop_1hl(fashionValidationParams, model_1hl_sigmoid3)\n",
    "    cost3 = neuralNetworkCostFunction(probs_sigmoid3, fashionValidationTarget)\n",
    "    acc3_train = accuracy(fashionTrainTarget, fashionTrainParams, model_1hl_3)\n",
    "    acc3_validation = accuracy(fashionValidationTarget, fashionValidationParams, model_1hl_3)\n",
    "    print(\"Third model \", activation, \"validation cost: \", cost3, \" acc_train: \", acc3_train, \" acc_validation: \", acc3_validation)\n",
    "    \n",
    "    \n",
    "    avg_loss = ((cost1+cost2+cost3)/3)\n",
    "    avg_acc = ((acc1_validation + acc2_validation + acc3_validation)/3)\n",
    "    print(\"Average validation loss: \", avg_loss, \" Average validation accuracy: \", avg_acc)\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sigmoid.50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since everything seems ok, we'll increase the number of epochs to 1000, but mantaining the current amount of neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning training with  500  epochs and  50  hidden neurons.\n",
      "Initialized weights\n",
      "Prepared for mini-batch.\n",
      "Ended iteration 0  Cost:  2.3133666884129656  Validation cost:  2.306578073772263\n",
      "Ended iteration 50  Cost:  2.099046573871898  Validation cost:  2.1411619192621494\n",
      "Ended iteration 100  Cost:  1.7478476886860745  Validation cost:  1.809657373888015\n",
      "Ended iteration 150  Cost:  1.5091591987292292  Validation cost:  1.5717407799731518\n",
      "Ended iteration 200  Cost:  1.3419038386592752  Validation cost:  1.4060697572935763\n",
      "Ended iteration 250  Cost:  1.2216475497294557  Validation cost:  1.2899136860604767\n",
      "Ended iteration 300  Cost:  1.1387032144696794  Validation cost:  1.2113086619234827\n",
      "Ended iteration 350  Cost:  1.0807978896577626  Validation cost:  1.1573967337104354\n",
      "Ended iteration 400  Cost:  1.039487567397365  Validation cost:  1.119327132533384\n",
      "Ended iteration 450  Cost:  1.0131137299819506  Validation cost:  1.0948955929231268\n",
      "======================First model trained=====================\n",
      "Beginning training with  500  epochs and  50  hidden neurons.\n",
      "Initialized weights\n",
      "Prepared for mini-batch.\n",
      "Ended iteration 0  Cost:  2.3129151866791067  Validation cost:  2.309525039908267\n",
      "Ended iteration 50  Cost:  2.1588149323338017  Validation cost:  2.1722213237489765\n",
      "Ended iteration 100  Cost:  1.821450636320456  Validation cost:  1.8453867365928915\n",
      "Ended iteration 150  Cost:  1.5214942174097466  Validation cost:  1.5595148364232416\n",
      "Ended iteration 200  Cost:  1.3361223827752182  Validation cost:  1.38391050680963\n",
      "Ended iteration 250  Cost:  1.2114018506264799  Validation cost:  1.2670172748687065\n",
      "Ended iteration 300  Cost:  1.1218286389716192  Validation cost:  1.1849718256015727\n",
      "Ended iteration 350  Cost:  1.0572574763436686  Validation cost:  1.1277648404426512\n",
      "Ended iteration 400  Cost:  1.0134219267370201  Validation cost:  1.0901680345718776\n"
     ]
    }
   ],
   "source": [
    "epochs = 500\n",
    "activation = \"sigmoid\"\n",
    "evaluate_neural_network_1hl(hidden_layer_1_neurons, epochs, fashionTrainParams, fashionTrainTarget, learning_rate, regularization_rate, activation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48000,)\n",
      "[5 7 8 5 2]\n"
     ]
    },
    {
     "ename": "AxisError",
     "evalue": "axis 1 is out of bounds for array of dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAxisError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-156-131d9e37f27d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfashionTrainTarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfashionTrainTarget\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mhue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfashionTrainTarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36margmax\u001b[0;34m(a, axis, out)\u001b[0m\n\u001b[1;32m   1002\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1003\u001b[0m     \"\"\"\n\u001b[0;32m-> 1004\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'argmax'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;31m# An AttributeError occurs if the object does not have\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAxisError\u001b[0m: axis 1 is out of bounds for array of dimension 1"
     ]
    }
   ],
   "source": [
    "print(fashionTrainTarget.shape)\n",
    "print(fashionTrainTarget[:5])\n",
    "hue = np.argmax(fashionTrainTarget, axis=1)\n",
    "print(hue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for 2000 epochs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "epochs=2000\n",
    "model_1hl_sigmoid1 = train_neural_network_1hl(hidden_layer_1_neurons, epochs, fashionTrainParams, fashionTrainTarget, learning_rate, regularization_rate)\n",
    "print(\"======================First model trained=====================\")\n",
    "model_1hl_sigmoid2 = train_neural_network_1hl(hidden_layer_1_neurons, epochs, fashionTrainParams, fashionTrainTarget, learning_rate, regularization_rate)\n",
    "print(\"======================Second model trained=====================\")\n",
    "model_1hl_sigmoid3 = train_neural_network_1hl(hidden_layer_1_neurons, epochs, fashionTrainParams, fashionTrainTarget, learning_rate, regularization_rate)\n",
    "print(\"======================Third model trained=====================\")\n",
    "probs_sigmoid1 = forward_prop_1hl(fashionValidationParams, model_1hl_sigmoid1)\n",
    "cost1 = neuralNetworkCostFunction(probs_sigmoid1, fashionValidationTarget)\n",
    "print(\"First model sigmoid validation cost: \", cost1)\n",
    "probs_sigmoid2 = forward_prop_1hl(fashionValidationParams, model_1hl_sigmoid2)\n",
    "cost2 = neuralNetworkCostFunction(probs_sigmoid2, fashionValidationTarget)\n",
    "print(\"Second model sigmoid validation cost: \", cost2)\n",
    "probs_sigmoid3 = forward_prop_1hl(fashionValidationParams, model_1hl_sigmoid3)\n",
    "cost3 = neuralNetworkCostFunction(probs_sigmoid3, fashionValidationTarget)\n",
    "avg = ((cost1+cost2+cost3)/3)\n",
    "print(\"Third model sigmoid validation cost: \", cost3)\n",
    "print(\"Average: \", avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From these trainings, we see that we achieve best performance generally around 1500 epochs. Now we'll test with other activation functions.\n",
    "\n",
    "## Relu.50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "epochs = 1500\n",
    "model_1hl_relu1 = train_neural_network_1hl(hidden_layer_1_neurons, epochs, fashionTrainParams, fashionTrainTarget, learning_rate, regularization_rate, activation=\"relu\")\n",
    "print(\"======================First model trained=====================\")\n",
    "model_1hl_relu2 = train_neural_network_1hl(hidden_layer_1_neurons, epochs, fashionTrainParams, fashionTrainTarget, learning_rate, regularization_rate, activation=\"relu\")\n",
    "print(\"======================Second model trained=====================\")\n",
    "model_1hl_relu3 = train_neural_network_1hl(hidden_layer_1_neurons, epochs, fashionTrainParams, fashionTrainTarget, learning_rate, regularization_rate, activation=\"relu\")\n",
    "print(\"======================Third model trained=====================\")\n",
    "probs_sigmoid1 = forward_prop_1hl(fashionValidationParams, model_1hl_sigmoid1)\n",
    "cost1 = neuralNetworkCostFunction(probs_sigmoid1, fashionValidationTarget)\n",
    "print(\"First model relu validation cost: \", cost1)\n",
    "probs_sigmoid2 = forward_prop_1hl(fashionValidationParams, model_1hl_sigmoid2)\n",
    "cost2 = neuralNetworkCostFunction(probs_sigmoid2, fashionValidationTarget)\n",
    "print(\"Second model relu validation cost: \", cost2)\n",
    "probs_sigmoid3 = forward_prop_1hl(fashionValidationParams, model_1hl_sigmoid3)\n",
    "cost3 = neuralNetworkCostFunction(probs_sigmoid3, fashionValidationTarget)\n",
    "avg = ((cost1+cost2+cost3)/3)\n",
    "print(\"Third model relu validation cost: \", cost3)\n",
    "print(\"Average: \", avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leaky-relu.50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "epochs = 2000\n",
    "model_1hl_leaky_relu1 = train_neural_network_1hl(hidden_layer_1_neurons, epochs, fashionTrainParams, fashionTrainTarget, learning_rate, regularization_rate, activation=\"leaky-relu\")\n",
    "print(\"======================First model trained=====================\")\n",
    "model_1hl_leaky_relu2 = train_neural_network_1hl(hidden_layer_1_neurons, epochs, fashionTrainParams, fashionTrainTarget, learning_rate, regularization_rate, activation=\"leaky-relu\")\n",
    "print(\"======================Second model trained=====================\")\n",
    "model_1hl_leaky_relu3 = train_neural_network_1hl(hidden_layer_1_neurons, epochs, fashionTrainParams, fashionTrainTarget, learning_rate, regularization_rate, activation=\"leaky-relu\")\n",
    "print(\"======================Third model trained=====================\")\n",
    "probs_sigmoid1 = forward_prop_1hl(fashionValidationParams, model_1hl_sigmoid1)\n",
    "cost1 = neuralNetworkCostFunction(probs_sigmoid1, fashionValidationTarget)\n",
    "print(\"First model leaky-relu validation cost: \", cost1)\n",
    "probs_sigmoid2 = forward_prop_1hl(fashionValidationParams, model_1hl_sigmoid2)\n",
    "cost2 = neuralNetworkCostFunction(probs_sigmoid2, fashionValidationTarget)\n",
    "print(\"Second model leaky-relu validation cost: \", cost2)\n",
    "probs_sigmoid3 = forward_prop_1hl(fashionValidationParams, model_1hl_sigmoid3)\n",
    "cost3 = neuralNetworkCostFunction(probs_sigmoid3, fashionValidationTarget)\n",
    "avg = ((cost1+cost2+cost3)/3)\n",
    "print(\"Third model leaky-relu validation cost: \", cost3)\n",
    "print(\"Average: \", avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Changing hidden layer size\n",
    "Now we will experiment changing the amount of neurons on the hidden layer and see the impact on different activation functions. We will fix the amount of epochs to 1350, which generates an average-to-good result for all activation functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_layer_1_neurons = 100\n",
    "epochs = 1350"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sigmoid.100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_1hl_sigmoid1 = train_neural_network_1hl(hidden_layer_1_neurons, epochs, fashionTrainParams, fashionTrainTarget, learning_rate, regularization_rate)\n",
    "print(\"======================First model trained=====================\")\n",
    "model_1hl_sigmoid2 = train_neural_network_1hl(hidden_layer_1_neurons, epochs, fashionTrainParams, fashionTrainTarget, learning_rate, regularization_rate)\n",
    "print(\"======================Second model trained=====================\")\n",
    "model_1hl_sigmoid3 = train_neural_network_1hl(hidden_layer_1_neurons, epochs, fashionTrainParams, fashionTrainTarget, learning_rate, regularization_rate)\n",
    "print(\"======================Third model trained=====================\")\n",
    "probs_sigmoid1 = forward_prop_1hl(fashionValidationParams, model_1hl_sigmoid1)\n",
    "cost1 = neuralNetworkCostFunction(probs_sigmoid1, fashionValidationTarget)\n",
    "print(\"First model sigmoid validation cost: \", cost1)\n",
    "probs_sigmoid2 = forward_prop_1hl(fashionValidationParams, model_1hl_sigmoid2)\n",
    "cost2 = neuralNetworkCostFunction(probs_sigmoid2, fashionValidationTarget)\n",
    "print(\"Second model sigmoid validation cost: \", cost2)\n",
    "probs_sigmoid3 = forward_prop_1hl(fashionValidationParams, model_1hl_sigmoid3)\n",
    "cost3 = neuralNetworkCostFunction(probs_sigmoid3, fashionValidationTarget)\n",
    "avg = ((cost1+cost2+cost3)/3)\n",
    "print(\"Third model sigmoid validation cost: \", cost3)\n",
    "print(\"Average: \", avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relu.100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1hl_relu1 = train_neural_network_1hl(hidden_layer_1_neurons, epochs, fashionTrainParams, fashionTrainTarget, learning_rate, regularization_rate, activation=\"relu\")\n",
    "print(\"======================First model trained=====================\")\n",
    "model_1hl_relu2 = train_neural_network_1hl(hidden_layer_1_neurons, epochs, fashionTrainParams, fashionTrainTarget, learning_rate, regularization_rate, activation=\"relu\")\n",
    "print(\"======================Second model trained=====================\")\n",
    "model_1hl_relu3 = train_neural_network_1hl(hidden_layer_1_neurons, epochs, fashionTrainParams, fashionTrainTarget, learning_rate, regularization_rate, activation=\"relu\")\n",
    "print(\"======================Third model trained=====================\")\n",
    "probs_sigmoid1 = forward_prop_1hl(fashionValidationParams, model_1hl_sigmoid1)\n",
    "cost1 = neuralNetworkCostFunction(probs_sigmoid1, fashionValidationTarget)\n",
    "print(\"First model relu validation cost: \", cost1)\n",
    "probs_sigmoid2 = forward_prop_1hl(fashionValidationParams, model_1hl_sigmoid2)\n",
    "cost2 = neuralNetworkCostFunction(probs_sigmoid2, fashionValidationTarget)\n",
    "print(\"Second model relu validation cost: \", cost2)\n",
    "probs_sigmoid3 = forward_prop_1hl(fashionValidationParams, model_1hl_sigmoid3)\n",
    "cost3 = neuralNetworkCostFunction(probs_sigmoid3, fashionValidationTarget)\n",
    "avg = ((cost1+cost2+cost3)/3)\n",
    "print(\"Third model relu validation cost: \", cost3)\n",
    "print(\"Average: \", avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leaky Relu.100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1hl_leaky_relu1 = train_neural_network_1hl(hidden_layer_1_neurons, epochs, fashionTrainParams, fashionTrainTarget, learning_rate, regularization_rate, activation=\"leaky-relu\")\n",
    "print(\"======================First model trained=====================\")\n",
    "model_1hl_leaky_relu2 = train_neural_network_1hl(hidden_layer_1_neurons, epochs, fashionTrainParams, fashionTrainTarget, learning_rate, regularization_rate, activation=\"leaky-relu\")\n",
    "print(\"======================Second model trained=====================\")\n",
    "model_1hl_leaky_relu3 = train_neural_network_1hl(hidden_layer_1_neurons, epochs, fashionTrainParams, fashionTrainTarget, learning_rate, regularization_rate, activation=\"leaky-relu\")\n",
    "print(\"======================Third model trained=====================\")\n",
    "probs_sigmoid1 = forward_prop_1hl(fashionValidationParams, model_1hl_sigmoid1)\n",
    "cost1 = neuralNetworkCostFunction(probs_sigmoid1, fashionValidationTarget)\n",
    "print(\"First model leaky-relu validation cost: \", cost1)\n",
    "probs_sigmoid2 = forward_prop_1hl(fashionValidationParams, model_1hl_sigmoid2)\n",
    "cost2 = neuralNetworkCostFunction(probs_sigmoid2, fashionValidationTarget)\n",
    "print(\"Second model leaky-relu validation cost: \", cost2)\n",
    "probs_sigmoid3 = forward_prop_1hl(fashionValidationParams, model_1hl_sigmoid3)\n",
    "cost3 = neuralNetworkCostFunction(probs_sigmoid3, fashionValidationTarget)\n",
    "avg = ((cost1+cost2+cost3)/3)\n",
    "print(\"Third model leaky-relu validation cost: \", cost3)\n",
    "print(\"Average: \", avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expanding to 500 neurons on the hidden layer..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_layer_1_neurons = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sigmoid.500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1hl_sigmoid1 = train_neural_network_1hl(hidden_layer_1_neurons, epochs, fashionTrainParams, fashionTrainTarget, learning_rate, regularization_rate)\n",
    "print(\"======================First model trained=====================\")\n",
    "model_1hl_sigmoid2 = train_neural_network_1hl(hidden_layer_1_neurons, epochs, fashionTrainParams, fashionTrainTarget, learning_rate, regularization_rate)\n",
    "print(\"======================Second model trained=====================\")\n",
    "model_1hl_sigmoid3 = train_neural_network_1hl(hidden_layer_1_neurons, epochs, fashionTrainParams, fashionTrainTarget, learning_rate, regularization_rate)\n",
    "print(\"======================Third model trained=====================\")\n",
    "probs_sigmoid1 = forward_prop_1hl(fashionValidationParams, model_1hl_sigmoid1)\n",
    "cost1 = neuralNetworkCostFunction(probs_sigmoid1, fashionValidationTarget)\n",
    "print(\"First model sigmoid validation cost: \", cost1)\n",
    "probs_sigmoid2 = forward_prop_1hl(fashionValidationParams, model_1hl_sigmoid2)\n",
    "cost2 = neuralNetworkCostFunction(probs_sigmoid2, fashionValidationTarget)\n",
    "print(\"Second model sigmoid validation cost: \", cost2)\n",
    "probs_sigmoid3 = forward_prop_1hl(fashionValidationParams, model_1hl_sigmoid3)\n",
    "cost3 = neuralNetworkCostFunction(probs_sigmoid3, fashionValidationTarget)\n",
    "avg = ((cost1+cost2+cost3)/3)\n",
    "print(\"Third model sigmoid validation cost: \", cost3)\n",
    "print(\"Average: \", avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relu.500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1hl_relu1 = train_neural_network_1hl(hidden_layer_1_neurons, epochs, fashionTrainParams, fashionTrainTarget, learning_rate, regularization_rate, activation=\"relu\")\n",
    "print(\"======================First model trained=====================\")\n",
    "model_1hl_relu2 = train_neural_network_1hl(hidden_layer_1_neurons, epochs, fashionTrainParams, fashionTrainTarget, learning_rate, regularization_rate, activation=\"relu\")\n",
    "print(\"======================Second model trained=====================\")\n",
    "model_1hl_relu3 = train_neural_network_1hl(hidden_layer_1_neurons, epochs, fashionTrainParams, fashionTrainTarget, learning_rate, regularization_rate, activation=\"relu\")\n",
    "print(\"======================Third model trained=====================\")\n",
    "probs_sigmoid1 = forward_prop_1hl(fashionValidationParams, model_1hl_sigmoid1)\n",
    "cost1 = neuralNetworkCostFunction(probs_sigmoid1, fashionValidationTarget)\n",
    "print(\"First model relu validation cost: \", cost1)\n",
    "probs_sigmoid2 = forward_prop_1hl(fashionValidationParams, model_1hl_sigmoid2)\n",
    "cost2 = neuralNetworkCostFunction(probs_sigmoid2, fashionValidationTarget)\n",
    "print(\"Second model relu validation cost: \", cost2)\n",
    "probs_sigmoid3 = forward_prop_1hl(fashionValidationParams, model_1hl_sigmoid3)\n",
    "cost3 = neuralNetworkCostFunction(probs_sigmoid3, fashionValidationTarget)\n",
    "avg = ((cost1+cost2+cost3)/3)\n",
    "print(\"Third model relu validation cost: \", cost3)\n",
    "print(\"Average: \", avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leaky Relu.500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1hl_leaky_relu1 = train_neural_network_1hl(hidden_layer_1_neurons, epochs, fashionTrainParams, fashionTrainTarget, learning_rate, regularization_rate, activation=\"leaky-relu\")\n",
    "print(\"======================First model trained=====================\")\n",
    "model_1hl_leaky_relu2 = train_neural_network_1hl(hidden_layer_1_neurons, epochs, fashionTrainParams, fashionTrainTarget, learning_rate, regularization_rate, activation=\"leaky-relu\")\n",
    "print(\"======================Second model trained=====================\")\n",
    "model_1hl_leaky_relu3 = train_neural_network_1hl(hidden_layer_1_neurons, epochs, fashionTrainParams, fashionTrainTarget, learning_rate, regularization_rate, activation=\"leaky-relu\")\n",
    "print(\"======================Third model trained=====================\")\n",
    "probs_sigmoid1 = forward_prop_1hl(fashionValidationParams, model_1hl_sigmoid1)\n",
    "cost1 = neuralNetworkCostFunction(probs_sigmoid1, fashionValidationTarget)\n",
    "print(\"First model leaky-relu validation cost: \", cost1)\n",
    "probs_sigmoid2 = forward_prop_1hl(fashionValidationParams, model_1hl_sigmoid2)\n",
    "cost2 = neuralNetworkCostFunction(probs_sigmoid2, fashionValidationTarget)\n",
    "print(\"Second model leaky-relu validation cost: \", cost2)\n",
    "probs_sigmoid3 = forward_prop_1hl(fashionValidationParams, model_1hl_sigmoid3)\n",
    "cost3 = neuralNetworkCostFunction(probs_sigmoid3, fashionValidationTarget)\n",
    "avg = ((cost1+cost2+cost3)/3)\n",
    "print(\"Third model leaky-relu validation cost: \", cost3)\n",
    "print(\"Average: \", avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
