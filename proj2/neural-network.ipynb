{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "Here, we'll explore neural networks applied to the fashion-mnist problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import csv\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions\n",
    "\n",
    "def formatArray (dataFrame, columnToExtract) :\n",
    "    array = dataFrame.values\n",
    "    target = array[:,columnToExtract]\n",
    "    params = np.delete(array, columnToExtract, axis = 1)\n",
    "    return params, target\n",
    "\n",
    "def loadFashionTrainData():\n",
    "    return pd.read_csv(\"fashion-mnist-dataset/fashion-mnist_train.csv\")\n",
    "\n",
    "def loadFashionTestData():\n",
    "    return pd.read_csv(\"fashion-mnist-dataset/fashion-mnist_test.csv\")\n",
    "\n",
    "def split_train_test(data, test_ratio):\n",
    "    np.random.seed(42)\n",
    "    shuffled_indices = np.random.permutation(len(data))\n",
    "    test_set_size = int(len(data) * test_ratio)\n",
    "    test_indices = shuffled_indices[:test_set_size]\n",
    "    train_indices = shuffled_indices[test_set_size:]\n",
    "    return data.iloc[train_indices], data.iloc[test_indices]\n",
    "\n",
    "def createTarget (target):\n",
    "    results = np.zeros((target.size, 10), dtype=int)\n",
    "    for i in range(10):\n",
    "        for j in range(target.size):\n",
    "            if (target[j] != i):\n",
    "                results[j][i - 1] = 0\n",
    "            else:\n",
    "                results[j][i - 1] = 1\n",
    "    return results\n",
    "\n",
    "def p_print(a):\n",
    "    for x in a:\n",
    "        print(*x, sep=\" \")\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The dataset\n",
    "First and foremost, we'll open train and test data. The training data is split to obtain validation items and the the target values are also separated from the original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "fashionTrainDataset = loadFashionTrainData()\n",
    "fashionTestDataset = loadFashionTestData()\n",
    "fashionTrain, fashionValidation = split_train_test(fashionTrainDataset, 0.2)\n",
    "fashionTrainParams, fashionTrainTarget = formatArray(fashionTrain, 0)\n",
    "fashionTrainParams = fashionTrainParams/255\n",
    "fashionValidationParams, fashionValidationTarget = formatArray(fashionValidation, 0)\n",
    "fashionValidationParams = fashionValidationParams/255\n",
    "print (fashionTrainParams[:5])\n",
    "print (type(fashionTrainParams))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation and Softmax Functions\n",
    "First, we'll start by implementing some useful functions seen in class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(n):\n",
    "    return (1/(1+np.exp(-n)))\n",
    "\n",
    "def derivative_sigmoid(n):\n",
    "    x = sigmoid(n)\n",
    "    return x * (1 - x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(n):\n",
    "    if n < 0:\n",
    "        return 0\n",
    "    return n\n",
    "\n",
    "def derivative_relu(n):\n",
    "    if n < 0:\n",
    "        return 0\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leaky_relu(n):\n",
    "    if n > 0:\n",
    "        return n\n",
    "    return 0.01 * n\n",
    "\n",
    "def derivative_leaky_relu(n):\n",
    "    if n < 0:\n",
    "        return 0.01\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(n):\n",
    "    exp = np.exp(n)\n",
    "    test_sum = np.sum(exp, axis=1, keepdims=True)\n",
    "    return exp/test_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Propagation\n",
    "In this section, we define forward propagation related functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_1hl(input_dimension,hidden_layer_1_neurons, output_dimension):\n",
    "    neural_data = {}\n",
    "    np.random.seed(0)\n",
    "    neural_data['w1'] = np.random.randn(input_dimension, hidden_layer_1_neurons)/ np.sqrt(input_dimension)\n",
    "    neural_data['w2'] = np.random.randn(hidden_layer_1_neurons, output_dimension)/ np.sqrt(hidden_layer_1_neurons)\n",
    "    \n",
    "    neural_data['b1'] = np.zeros((1, hidden_layer_1_neurons))\n",
    "    neural_data['b2'] = np.zeros((1, output_dimension))\n",
    "    return neural_data\n",
    "\n",
    "def initialize_2hl(input_dimension, hidden_layer_1_neurons, hidden_layer_2_neurons, output_dimension):\n",
    "    neural_data = {}\n",
    "    np.random.seed(0)\n",
    "\n",
    "    neural_data['w1'] = np.random.randn(input_dimension, hidden_layer_1_neurons)/ np.sqrt(input_dimension)\n",
    "    neural_data['w2'] = np.random.randn(hidden_layer_1_neurons, hidden_layer_2_neurons)/ np.sqrt(hidden_layer_1_neurons)\n",
    "    neural_data['w3'] = np.random.randn(hidden_layer_2_neurons, output_dimension)/ np.sqrt(hidden_layer_2_neurons)\n",
    "\n",
    "    neural_data['b1'] = np.zeros((1, hidden_layer_1_neurons))\n",
    "    neural_data['b2'] = np.zeros((1, hidden_layer_2_neurons))\n",
    "    neural_data['b3'] = np.zeros((1, output_dimension))\n",
    "    return neural_data\n",
    "    \n",
    "def forward_prop_1hl(x, neural_data):\n",
    "    w1 , w2, b1, b2 = neural_data['w1'], neural_data['w2'], neural_data['b1'], neural_data['b2']\n",
    "    x1 = np.dot(x, w1) + b1 #Output of hidden layer\n",
    "    y1 = np.asarray([[relu(n) for n in j] for j in x1]) #Output of hidden layer with activation function\n",
    "    x2 = np.dot(y1, w2) + b2 #Output of last layer\n",
    "    neural_data['x1'] = x1\n",
    "    neural_data['x2'] = x2\n",
    "    neural_data['y1'] = y1\n",
    "#     print(\"@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\")\n",
    "#     print(\"@@@@@@@@@@@@@@@@@@@ Y1 @@@@@@@@@@@@@@@@\")\n",
    "#     print(\"@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\")\n",
    "#     print(y1)\n",
    "#     print(\"@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\")\n",
    "#     print(\"@@@@@@@@@@@@@@@@@@ OUT @@@@@@@@@@@@@@@@\")\n",
    "#     print(\"@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\")\n",
    "    neural_data['o'] = softmax(x2)\n",
    "#     print(neural_data['o'])\n",
    "#     print(\"@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\")\n",
    "#     print(\"@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\")\n",
    "    return neural_data['o']\n",
    "\n",
    "def forward_prop_2hl(x, neural_data):\n",
    "    w1 , w2, w3, b1, b2, b3 = neural_data['w1'], neural_data['w2'], neural_data['w3'], neural_data['b1'], neural_data['b2'], neural_data['b3']\n",
    "\n",
    "    x1 = np.dot(x, w1) + b1\n",
    "    y1 = np.asarray([[relu(n) for n in j] for j in x1])\n",
    "    x2 = np.dot(a1, w2) + b2\n",
    "    y2 = np.asarray([[relu(n) for n in j] for j in x2])\n",
    "    x3 = np.dot(a2, w3) + b3\n",
    "    \n",
    "    neural_data['x1'] = x1\n",
    "    neural_data['x2'] = x2\n",
    "    neural_data['x3'] = x3\n",
    "\n",
    "    neural_data['y1'] = y1\n",
    "    neural_data['y2'] = y2\n",
    "    \n",
    "    neural_data['o'] = softmax(x3)\n",
    "    return neural_data['o']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction Functions\n",
    "Helper functions that return predictions, given our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_1hl(x, neural_data):\n",
    "    test = forward_prop_1hl(x,neural_data)\n",
    "    return np.argmax(test, axis=1)\n",
    "\n",
    "def predict_2hl(x, neural_data):\n",
    "    return np.argmax(forward_prop_2hl(x,neural_data), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_cost(fashionTrainOutput, fashionTarget, fashionTargetMinusOne, testCasesAmount):\n",
    "    final_cost = 0\n",
    "    for j in testCasesAmount:\n",
    "        cost = np.add(np.dot(fashionTarget, np.log10(fashionTrainOutput[j])),np.dot(fashionTargetMinusOne, (np.ones(10) - np.log10(fashionTrainOutput[j]))))\n",
    "    final_cost = np.sum(cost)/testCasesAmount\n",
    "    return final_cost\n",
    "\n",
    "def neuralNetworkCostFunction(fashionTrainOutput, fashionTarget):\n",
    "    diference = fashionTrainOutput - fashionTarget\n",
    "    squareDiference = diference ** 2\n",
    "    print(fashionTrainOutput)\n",
    "    n = fashionTrainOutput.shape[0]    \n",
    "    return (np.sum(squareDiference)/(2*n))\n",
    "\n",
    "def regressionLogisticCostFunction (results, model, X):\n",
    "    agaTheta = model.predict_proba(X)\n",
    "    n = X.shape[0]\n",
    "    diference = results - agaTheta\n",
    "    squareDiference = diference * diference\n",
    "    return (np.sum(squareDiference)/(2*n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and backward propagation\n",
    "\n",
    "Some notes:\n",
    "\n",
    "### For one hidden layer:\n",
    "- x1 = #Output of hidden layer\n",
    "- x2 = #Output of last layer\n",
    "- y1 = #Output of hidden layer with activation function\n",
    "- o = Final output with Softmax\n",
    "\n",
    "### For TWO hidden layers:\n",
    "- x1 = #Output of first hidden layer\n",
    "- x2 = #Output of second hidden layer\n",
    "- x3 = Output of last layer\n",
    "- y1 = #Output of first hidden layer with activation function\n",
    "- y2 = #Output of second hidden layer with activation function\n",
    "- o = Final output with Softmax\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One hidden layer\n",
    "\n",
    "Here, we present our code and results achieved by a learning algorithm that uses a neural network with only one hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_neural_network_1hl(hidden_layer_1_neurons, epochs, trainParams, trainTarget, learning_rate, regularization_rate):\n",
    "    input_dimension = 784\n",
    "    output_dimension = 10\n",
    "#     Initializes weights and biases for our neural network\n",
    "    neural_data = initialize_1hl(input_dimension, hidden_layer_1_neurons, output_dimension)\n",
    "    sigmoid = True\n",
    "    relu = False\n",
    "    leaky_relu = False\n",
    "    \n",
    "#     Prepares for mini-batch\n",
    "    batchSize = 5\n",
    "    start_idx = 0;\n",
    "    indices = np.arange(trainParams.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "#     Performs Backpropagation\n",
    "    for j in range(epochs):\n",
    "        excerpt = indices[start_idx:start_idx + batchSize]\n",
    "        mini_batch_data = trainParams[excerpt]\n",
    "        miniBatchTarget = createTarget(trainTarget[excerpt])\n",
    "#         Performs Forward propagation\n",
    "        probs = forward_prop_1hl(mini_batch_data, neural_data)\n",
    "    \n",
    "#         Calculates cost\n",
    "#         cur_result = np.amax(probs, axis=1)\n",
    "#         print(\"============================\")\n",
    "#         print(\"============================\")\n",
    "#         print(\"============================\")\n",
    "#         print(probs)\n",
    "#         print(\"============================\")\n",
    "#         neuralNetworkCostFunction(fashionTrainOutput, fashionTarget)\n",
    "\n",
    "#         print(\"============================\")\n",
    "#         print(\"============================\")\n",
    "#         print(\"============================\")\n",
    "#         Performs Backward propagation\n",
    "\n",
    "#         Correct from here\n",
    "\n",
    "        delta3 = probs - miniBatchTarget\n",
    "#         To here\n",
    "        print(\"============================\")\n",
    "        print(\"============================\")\n",
    "        print(\"============================\")\n",
    "        \n",
    "        p_print(neural_data['y1'].T)\n",
    "        print(\"============================\")\n",
    "        p_print(delta3)\n",
    "        print(\"============================\")\n",
    "        dW2 = (neural_data['y1'].T).dot(delta3)\n",
    "        p_print(dW2)\n",
    "        print(\"============================\")\n",
    "        print(\"============================\")\n",
    "        print(\"============================\")\n",
    "        db2 = np.sum(delta3, axis=0, keepdims=True)\n",
    "        delta2 = delta3.dot(neural_data['w2'].T)\n",
    "#         Correct from here..\n",
    "        aux = neural_data['y1']\n",
    "        if sigmoid:\n",
    "            aux = [[derivative_sigmoid(n) for n in x] for x in aux]\n",
    "        if relu:\n",
    "            aux = [[derivative_relu(n) for n in x] for x in aux]\n",
    "        if leaky_relu:\n",
    "            aux = [[derivative_leaky_relu(n) for n in x] for x in aux]\n",
    "\n",
    "        delta2 = delta2 * aux\n",
    "\n",
    "#         to here\n",
    "        dW1 = np.dot(mini_batch_data.T, delta2)\n",
    "        db1 = np.sum(delta2, axis=0)\n",
    "        \n",
    "        \n",
    "         # Performs regularization\n",
    "        dW2 += regularization_rate * neural_data['w2']\n",
    "        dW1 += regularization_rate * neural_data['w1']\n",
    "\n",
    "        # Gradient descent parameter update\n",
    "        neural_data['w1'] += -learning_rate * dW1\n",
    "        neural_data['b1'] += -learning_rate * db1\n",
    "        neural_data['w2'] += -learning_rate * dW2\n",
    "        neural_data['b2'] += -learning_rate * db2\n",
    "        \n",
    "        print(\"Ended iteration\", j)\n",
    "        start_idx += batchSize;\n",
    "        start_idx %= mini_batch_data.shape[0]\n",
    "        \n",
    "    return neural_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Two hidden layers\n",
    "\n",
    "Same as before, but for 2 hidden layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_neural_network_2hl(hidden_layer_1_neurons, hidden_layer_2_neurons, epochs, trainParams, trainTarget, learning_rate, regularization_rate):\n",
    "    input_dimension = 784\n",
    "    output_dimension = 10\n",
    "#     Initializes weights and biases for our neural network\n",
    "    neural_data = initialize_2hl(input_dimension, hidden_layer_1_neurons, hidden_layer_2_neurons, output_dimension)\n",
    "    sigmoid = True\n",
    "    relu = False\n",
    "    leaky_relu = False\n",
    "    \n",
    "#     Prepares for mini-batch\n",
    "    batchSize = 64\n",
    "    start_idx = 0;\n",
    "    indices = np.arange(trainParams.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "    excerpt = indices[start_idx:start_idx + batchSize]\n",
    "    mini_batch_data = trainParams[excerpt]\n",
    "    miniBatchTarget = createTarget(trainTarget[excerpt])\n",
    "#     Performs Backpropagation\n",
    "    for j in range(epochs):\n",
    "\n",
    "#         Performs Forward propagation\n",
    "        probs = forward_prop_1hl(mini_batch_data, neural_data)\n",
    "        \n",
    "#         Calculates cost\n",
    "\n",
    "#         Performs Backward propagation\n",
    "        delta4 = probs - miniBatchTarget\n",
    "        dW3 = (neural_data['y2'].T).dot(delta4)\n",
    "        db3 = np.sum(delta4, axis=0, keepdims=True)\n",
    "        delta3 = delta4.dot(neural_data['w3'].T)\n",
    "        aux = neural_data['y2']\n",
    "        if sigmoid:\n",
    "            aux = [[derivative_sigmoid(n) for n in x] for x in aux]\n",
    "        if relu:\n",
    "            aux = [[derivative_relu(n) for n in x] for x in aux]\n",
    "        if leaky_relu:\n",
    "            aux = [[derivative_leaky_relu(n) for n in x] for x in aux]        \n",
    "        delta3 = delta3 * aux\n",
    "        dW2 = np.dot(mini_batch_data.T, delta3)\n",
    "        db2 = np.sum(delta3, axis=0)\n",
    "        delta2 = delta3.dot(neural_data['w2'].T)\n",
    "        aux = neural_data['y1']\n",
    "        if sigmoid:\n",
    "            aux = [[derivative_sigmoid(n) for n in x] for x in aux]\n",
    "        if relu:\n",
    "            aux = [[derivative_relu(n) for n in x] for x in aux]\n",
    "        if leaky_relu:\n",
    "            aux = [[derivative_leaky_relu(n) for n in x] for x in aux]\n",
    "        delta2 = delta2 * aux\n",
    "        dW1 = np.dot(mini_batch_data.T, delta2)\n",
    "        db1 = np.sum(delta2, axis=0)      \n",
    "        \n",
    "         # Performs regularization\n",
    "        dW3 += regularization_rate * neural_data['w3']\n",
    "        dW2 += regularization_rate * neural_data['w2']\n",
    "        dW1 += regularization_rate * neural_data['w1']\n",
    "\n",
    "        # Gradient descent parameter update\n",
    "        \n",
    "        neural_data['w1'] += -learning_rate * dW1\n",
    "        neural_data['b1'] += -learning_rate * db1\n",
    "        neural_data['w2'] += -learning_rate * dW2\n",
    "        neural_data['b2'] += -learning_rate * db2\n",
    "        neural_data['w3'] += -learning_rate * dW3\n",
    "        neural_data['b3'] += -learning_rate * db3\n",
    "        \n",
    "        print(\"Ended iteration\", j)\n",
    "        start_idx += 1;\n",
    "        start_idx %= mini_batch_data.shape[0]\n",
    "        \n",
    "    return neural_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the neural networks\n",
    "\n",
    "Now, we'll test our neural networks under multiple circumstances on the validation set, so we can gest the best possible models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem details\n",
    "- Input dimension: 28x28 = 784 neurons\n",
    "- Output dimension: 10 classes = 10 neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_layers = 1\n",
    "input_neurons = 784\n",
    "output_neurons = 10\n",
    "hidden_layer_1_neurons = 15\n",
    "hidden_layer_2_neurons = 15\n",
    "learning_rate = 1\n",
    "regularization_rate = 0\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================\n",
      "============================\n",
      "============================\n",
      "0.0 0.24262016844328663 0.0 0.0 0.0\n",
      "0.04851853246174441 0.0 0.15850424395748117 0.2000615618427743 0.0\n",
      "0.0 0.0 0.28772926558227496 0.29818285846172593 0.0\n",
      "0.0 0.0 0.0 0.0 0.0\n",
      "0.30362967932420926 0.17979462903548665 0.40297441702178094 0.0 0.01060942467278958\n",
      "0.0 0.0 0.06782969696553134 0.0 0.0\n",
      "0.0 0.0 0.0 0.0 0.047777011579303974\n",
      "0.0 0.39917641689704997 0.0 0.0 0.625271298343638\n",
      "0.0 0.0 0.0 0.0 0.12612483685936793\n",
      "0.02648186011921383 0.4745777448223156 0.18360061745620024 0.389226225085768 0.25262283288844045\n",
      "0.21928443710520437 0.44657241952810967 0.2609758241932211 0.20383821676243782 0.599338899480184\n",
      "0.2892781497399259 0.8280640087723181 0.8093085476986365 0.46018612859732344 1.2082204924739948\n",
      "0.0 0.0 0.0 0.08256547520662072 0.0\n",
      "0.0 0.049610556868801 0.0 0.0 0.0\n",
      "0.0 0.8564552119791182 0.17669927485589762 0.030833879504660627 0.0\n",
      "============================\n",
      "0.10906496260544486 0.10291645575274225 0.10318281466479211 0.09448640117417423 0.12566542603238537 0.07927362078405739 0.08960084109110796 0.10606015155064208 -0.8956116563278949 0.08536098267254849\n",
      "0.09999279107523565 -0.9115372709162992 0.08144627229987154 0.055483171821970906 0.2184911481999297 0.10452333169721524 0.05918189454484054 0.1627015673201924 0.06077505155056438 0.06894204240647882\n",
      "0.12992654947129684 0.09804369135406019 0.09810294525929558 0.07118137500177465 -0.8347320874147572 0.07534057827490286 0.07705757644805078 0.13495544537183712 0.08391119240585014 0.0662127338276891\n",
      "0.11016438238516382 0.10870003594502411 0.09673961882213565 0.07591050341242407 -0.8712231426755526 0.095786354433801 0.08428447974188552 0.1294121009065835 0.09643088831719403 0.07379477871134096\n",
      "0.09223561966594956 0.10728229410692036 0.09764142697597586 -0.9549275359508472 0.24230995885015422 0.08568378994773049 0.07546742591023599 0.11483146328695239 0.0856300191456072 0.053845538061321176\n",
      "============================\n",
      "0.024260267813788042 -0.2211573262120463 0.019760508304472624 0.013461336493214396 0.05301035917963405 0.02535946834263188 0.014358721223262037 0.039474681669211815 0.014745253244347358 0.0167267299414841\n",
      "0.047925239821983885 0.04228039553569028 0.039909891144765786 0.031053665424033625 -0.3005097390076962 0.034951208813405986 0.033423338869439906 0.052427280771241355 -0.010861368982177662 0.02940008760931303\n",
      "0.07023280109929411 0.06062252674125863 0.05707318445779744 0.043116175647182335 -0.4999606575108118 0.05023953822228049 0.04730390697234252 0.0774191013510299 0.052897743687492595 0.041055679332133804\n",
      "0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
      "0.10442916878683439 -0.09199371223490936 0.08654186378134841 0.05721749331796236 -0.2563656190929514 0.07413193902912865 0.06969895933968799 0.11705776542666474 -0.22628470308251153 0.0655668447287457\n",
      "0.00881287847841518 0.006650273873927988 0.006654293048364129 0.004828211095960223 -0.0566196245367484 0.005110328593594555 0.005226792059369549 0.009153986963420032 0.00569167075290521 0.004491189670791537\n",
      "0.004406742268804349 0.005125627407800629 0.004665015587250962 -0.04562358394251984 0.011576845709764487 0.004093715424491369 0.0036056080815736096 0.005486304151129143 0.004091146416255696 0.0025725788954495945\n",
      "0.09758704971900616 -0.29678364234696575 0.09356381296367813 -0.5749412065020997 0.23872597623399971 0.09529886361665724 0.07081163199113408 0.13674744680654108 0.07780196056305218 0.061188106954996754\n",
      "0.011633202482990598 0.01353096184213406 0.012315009048060842 -0.12044007968431882 0.030561304029375857 0.010806854028649858 0.00951831678112495 0.014483099573389368 0.01080007219500426 0.0067912597035890195\n",
      "0.14037688130539794 -0.34245813918381535 0.12171687296111232 -0.1697879420278734 -0.3241283028386377 0.12446457956591515 0.09647755041077592 0.18418074917136842 0.0796966996977376 0.08946105093801912\n",
      "0.18021406595111503 -0.27245672076366967 0.1628400780884638 -0.4927784872434756 -0.12507908932019088 0.15460132600309956 0.12859819840880446 0.22633757650492742 -0.07637674175509841 0.11409979412602426\n",
      "0.38163829552574147 -0.46604921240208347 0.33917741625865805 -0.9879458928668413 -0.5664398427534294 0.3180623718924957 0.2672571565861346 0.47292417280222565 0.006990896328453762 0.234384638628645\n",
      "0.009095774582474929 0.00897487012277767 0.00798735259935698 0.006267586787420597 -0.07193295278601253 0.00790864587213658 0.0069589881224315754 0.010684971608839218 0.007961862118505695 0.0060929009720692845\n",
      "0.004960698048108116 -0.04522187161682473 0.0040405949236846294 0.002752551050935345 0.010839467533102241 0.005185460691281247 0.0029360467449201953 0.00807171535818146 0.003015084151153587 0.003420253115457908\n",
      "0.1119940694450012 -0.7600149536125469 0.09007266144612934 0.06243716434547605 0.012768138699064624 0.10578564266470404 0.066901477410725 0.16718341178940704 0.06985149490008763 0.07302089291195199\n",
      "============================\n",
      "============================\n",
      "============================\n",
      "Ended iteration 0\n",
      "============================\n",
      "============================\n",
      "============================\n",
      "14.01691870028656 37.174263978404944 14.040168848810543 13.337418163249076 19.440895275564802\n",
      "0.0 0.0 0.0 0.0 0.0\n",
      "0.0 0.0 0.0 0.0 0.0\n",
      "0.0 0.0 0.0 0.0 0.0\n",
      "9.843999676544856 19.69892647464385 10.30505405889645 9.109208063702845 14.478462557216398\n",
      "0.0 0.0 0.0 0.0 0.0\n",
      "1.0559584966534554 25.264123254807803 3.4326937108703737 3.415397529061524 18.7452224294128\n",
      "0.34389989822057143 0.0 2.2593230353313323 2.150196282547552 0.0\n",
      "8.104243828512816 22.515936410702754 0.41351476666243503 0.07717173265845331 10.023055760086029\n",
      "7.220743904964212 26.998943302844665 7.620896203017703 7.229645481358239 12.689091730430048\n",
      "2.212251145410097 0.0 3.368173844440048 3.140695128518437 0.0\n",
      "0.0 1.3489904431917705 9.964282738677252 9.490223153427046 0.0\n",
      "0.0 3.0644777421677745 0.0 0.0 3.6064690432737603\n",
      "5.056107385750792 6.2345370249561425 0.7585687113925843 0.4769495427030213 1.2622887076165379\n",
      "0.0 0.0 0.0 0.0 0.0\n",
      "============================\n",
      "4.1858651531701976e-13 0.9991216344981863 9.922174695314219e-14 5.7612168073019905e-09 1.2452815180155265e-05 6.604168224805045e-12 1.811883592052076e-15 1.7945461964412807e-10 -0.9991340932882037 2.704187723017535e-11\n",
      "4.145488917692186e-39 0.0 2.510694532791137e-41 7.414809261341909e-26 4.2158624472795485e-20 4.693801868674216e-33 3.990286242016269e-44 1.1204845326995219e-27 3.183207140785075e-21 2.002556191511107e-36\n",
      "1.37219315278824e-17 0.9350323040584937 2.0969288718541793e-17 8.794512167365234e-08 -0.9350323952586702 2.6198002969095463e-16 5.887471637335831e-19 2.2835412381975346e-13 3.2548256350957752e-09 5.490912759109921e-16\n",
      "9.692218206600363e-17 0.9263448731269631 1.6205320746202091e-16 2.3369466303801664e-07 -0.9263451124193276 2.7372125565455427e-15 7.431175693037052e-18 1.2502251125132692e-12 5.596443564844542e-09 4.431612209944925e-15\n",
      "1.4755929113119683e-20 0.9999999998687906 2.1372779252537723e-22 -0.9999999999996299 1.2161133009108937e-10 5.519122226687049e-19 1.7884280234172303e-23 4.194954168573416e-14 9.185943617624135e-12 5.3095428398148254e-21\n",
      "============================\n",
      "5.868778791010486e-12 58.9285623598223 1.3932389482445238e-12 -19.440890843155298 -25.482890283895834 9.261028541209608e-11 2.5504404159114233e-14 2.536097256168897e-09 -14.004781235786286 3.7911061034112905e-10\n",
      "0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
      "0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
      "0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
      "4.1215900245985626e-12 42.387642230341 9.784311140453801e-13 -14.47845946544508 -18.073707160512534 6.50390714167296e-11 1.790994094992955e-14 1.7809003448667504e-09 -9.835475606500694 2.6624625767684544e-10\n",
      "0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
      "4.423883950059384e-13 26.173768906430766 1.0539950802419244e-13 -18.745221323273412 -6.373503478744062 6.983985862231466e-12 1.940675617005446e-15 1.9533686939137804e-10 -1.0550441046447334 2.857212070575295e-11\n",
      "1.4419126400559166e-13 4.447961154306762 3.4518171279748955e-14 7.031671168315035e-07 -4.104359763947204 2.2776502221214917e-12 6.404152392733676e-16 6.49185805522595e-11 -0.3436020936033615 9.310468237818171e-12\n",
      "3.39234048523018e-12 18.57831840301954 8.041384095788027e-13 -10.023055658990785 -0.45803643824280743 5.35221146782302e-11 1.4684763530476765e-14 1.4549553661848393e-09 -8.097226307517797 2.1915453575981125e-10\n",
      "3.023311502846413e-12 33.72642234326577 7.17786219275657e-13 -12.689089329074871 -13.82284166677358 4.7708800058888656e-11 1.3141359177544941e-14 1.307107100006032e-09 -7.2144913489711895 1.9529869380876088e-10\n",
      "9.263691188392256e-13 8.269026160916836 2.200830112757147e-13 1.0429234068124596e-06 -6.058691690561981 1.461955786423176e-11 4.033663611963127e-15 4.016944001643972e-10 -2.2103355137555654 5.983919165424762e-11\n",
      "1.056542341788049e-15 18.108145810443613 1.746865023163693e-15 3.0941245598393618e-06 -18.108148990125894 2.858720106770748e-14 7.638995882028024e-17 1.41403003640523e-11 8.554350118912882e-08 4.752828952434749e-14\n",
      "5.321680155120817e-20 3.606469042800558 7.708026674300099e-22 -3.6064690432724253 4.385874974140547e-10 1.990454345659113e-18 6.449910302577521e-23 1.5128972346912585e-13 3.312882129997449e-11 1.9148701885728024e-20\n",
      "2.1164750263146447e-12 7.4650609965238806 5.017690057161791e-13 -1.2622885003137596 -1.151043234010954 3.339288867698071e-11 9.16506893549617e-15 9.081642968116709e-10 -5.051729263280082 1.3672916537363653e-10\n",
      "0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
      "============================\n",
      "============================\n",
      "============================\n",
      "Ended iteration 1\n",
      "============================\n",
      "============================\n",
      "============================\n",
      "14.016860452591084 37.17415900322519 14.040083901221273 13.337334205309373 19.440814038292352\n",
      "7.032312513824535 0.0 0.0 0.0 0.0\n",
      "17.31669234900227 13.152458985821365 27.388131403288618 25.64383044984529 14.479899261721402\n",
      "0.0 0.0 0.0 0.0 0.0\n",
      "9.846429750004544 19.70289034609942 10.307976546431657 9.112146882257507 14.481409038404585\n",
      "0.0 0.0 0.0 0.0 0.0\n",
      "0.0 13.427175956573238 0.0 0.0 11.338953592827211\n",
      "0.28680975514785106 0.0 0.7575287495435985 0.622570797407073 0.0\n",
      "0.14629453223165617 4.852527400354333 0.0 0.0 0.0\n",
      "7.161046961545517 26.916360049695907 7.569496952653117 7.181741286530048 12.632594348299927\n",
      "11.081215325504136 22.668224506681995 13.684219426218167 13.029580501271308 28.25807895279393\n",
      "0.0 0.0 1.2994562986387297 1.9859816025879544 0.0\n",
      "0.0 0.0 0.0 0.0 1.045270855236193\n",
      "1.6001441719469 0.0 0.0 0.0 0.0\n",
      "0.0 0.0 0.0 0.0 0.0\n",
      "============================\n",
      "0.0 0.0 0.0 0.0 nan 0.0 0.0 0.0 -1.0 0.0\n",
      "0.0 -1.0 0.0 nan nan 0.0 0.0 0.0 nan 0.0\n",
      "0.0 0.0 0.0 0.0 nan 0.0 0.0 0.0 0.0 0.0\n",
      "0.0 0.0 0.0 0.0 nan 0.0 0.0 0.0 0.0 0.0\n",
      "0.0 0.0 0.0 nan nan 0.0 0.0 0.0 0.0 0.0\n",
      "============================\n",
      "0.0 -37.17415900322519 0.0 nan nan 0.0 0.0 0.0 nan 0.0\n",
      "0.0 0.0 0.0 nan nan 0.0 0.0 0.0 nan 0.0\n",
      "0.0 -13.152458985821365 0.0 nan nan 0.0 0.0 0.0 nan 0.0\n",
      "0.0 0.0 0.0 nan nan 0.0 0.0 0.0 nan 0.0\n",
      "0.0 -19.70289034609942 0.0 nan nan 0.0 0.0 0.0 nan 0.0\n",
      "0.0 0.0 0.0 nan nan 0.0 0.0 0.0 nan 0.0\n",
      "0.0 -13.427175956573238 0.0 nan nan 0.0 0.0 0.0 nan 0.0\n",
      "0.0 0.0 0.0 nan nan 0.0 0.0 0.0 nan 0.0\n",
      "0.0 -4.852527400354333 0.0 nan nan 0.0 0.0 0.0 nan 0.0\n",
      "0.0 -26.916360049695907 0.0 nan nan 0.0 0.0 0.0 nan 0.0\n",
      "0.0 -22.668224506681995 0.0 nan nan 0.0 0.0 0.0 nan 0.0\n",
      "0.0 0.0 0.0 nan nan 0.0 0.0 0.0 nan 0.0\n",
      "0.0 0.0 0.0 nan nan 0.0 0.0 0.0 nan 0.0\n",
      "0.0 0.0 0.0 nan nan 0.0 0.0 0.0 nan 0.0\n",
      "0.0 0.0 0.0 nan nan 0.0 0.0 0.0 nan 0.0\n",
      "============================\n",
      "============================\n",
      "============================\n",
      "Ended iteration 2\n",
      "============================\n",
      "============================\n",
      "============================\n",
      "nan nan nan nan nan\n",
      "nan nan nan nan nan\n",
      "nan nan nan nan nan\n",
      "nan nan nan nan nan\n",
      "nan nan nan nan nan\n",
      "nan nan nan nan nan\n",
      "nan nan nan nan nan\n",
      "nan nan nan nan nan\n",
      "nan nan nan nan nan\n",
      "nan nan nan nan nan\n",
      "nan nan nan nan nan\n",
      "nan nan nan nan nan\n",
      "nan nan nan nan nan\n",
      "nan nan nan nan nan\n",
      "nan nan nan nan nan\n",
      "============================\n",
      "nan nan nan nan nan nan nan nan nan nan\n",
      "nan nan nan nan nan nan nan nan nan nan\n",
      "nan nan nan nan nan nan nan nan nan nan\n",
      "nan nan nan nan nan nan nan nan nan nan\n",
      "nan nan nan nan nan nan nan nan nan nan\n",
      "============================\n",
      "nan nan nan nan nan nan nan nan nan nan\n",
      "nan nan nan nan nan nan nan nan nan nan\n",
      "nan nan nan nan nan nan nan nan nan nan\n",
      "nan nan nan nan nan nan nan nan nan nan\n",
      "nan nan nan nan nan nan nan nan nan nan\n",
      "nan nan nan nan nan nan nan nan nan nan\n",
      "nan nan nan nan nan nan nan nan nan nan\n",
      "nan nan nan nan nan nan nan nan nan nan\n",
      "nan nan nan nan nan nan nan nan nan nan\n",
      "nan nan nan nan nan nan nan nan nan nan\n",
      "nan nan nan nan nan nan nan nan nan nan\n",
      "nan nan nan nan nan nan nan nan nan nan\n",
      "nan nan nan nan nan nan nan nan nan nan\n",
      "nan nan nan nan nan nan nan nan nan nan\n",
      "nan nan nan nan nan nan nan nan nan nan\n",
      "============================\n",
      "============================\n",
      "============================\n",
      "Ended iteration 3\n",
      "============================\n",
      "============================\n",
      "============================\n",
      "nan nan nan nan nan\n",
      "nan nan nan nan nan\n",
      "nan nan nan nan nan\n",
      "nan nan nan nan nan\n",
      "nan nan nan nan nan\n",
      "nan nan nan nan nan\n",
      "nan nan nan nan nan\n",
      "nan nan nan nan nan\n",
      "nan nan nan nan nan\n",
      "nan nan nan nan nan\n",
      "nan nan nan nan nan\n",
      "nan nan nan nan nan\n",
      "nan nan nan nan nan\n",
      "nan nan nan nan nan\n",
      "nan nan nan nan nan\n",
      "============================\n",
      "nan nan nan nan nan nan nan nan nan nan\n",
      "nan nan nan nan nan nan nan nan nan nan\n",
      "nan nan nan nan nan nan nan nan nan nan\n",
      "nan nan nan nan nan nan nan nan nan nan\n",
      "nan nan nan nan nan nan nan nan nan nan\n",
      "============================\n",
      "nan nan nan nan nan nan nan nan nan nan\n",
      "nan nan nan nan nan nan nan nan nan nan\n",
      "nan nan nan nan nan nan nan nan nan nan\n",
      "nan nan nan nan nan nan nan nan nan nan\n",
      "nan nan nan nan nan nan nan nan nan nan\n",
      "nan nan nan nan nan nan nan nan nan nan\n",
      "nan nan nan nan nan nan nan nan nan nan\n",
      "nan nan nan nan nan nan nan nan nan nan\n",
      "nan nan nan nan nan nan nan nan nan nan\n",
      "nan nan nan nan nan nan nan nan nan nan\n",
      "nan nan nan nan nan nan nan nan nan nan\n",
      "nan nan nan nan nan nan nan nan nan nan\n",
      "nan nan nan nan nan nan nan nan nan nan\n",
      "nan nan nan nan nan nan nan nan nan nan\n",
      "nan nan nan nan nan nan nan nan nan nan\n",
      "============================\n",
      "============================\n",
      "============================\n",
      "Ended iteration 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nacholez/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:4: RuntimeWarning: invalid value encountered in true_divide\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "# fashionTrainParams, fashionTrainTarget\n",
    "model_1hl = train_neural_network_1hl(hidden_layer_1_neurons, epochs, fashionTrainParams, fashionTrainTarget, learning_rate, regularization_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results = predict_1hl(fashionValidationParams, model_1hl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(neuralNetworkCostFunction(results, fashionValidationTarget))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
