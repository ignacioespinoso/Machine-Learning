{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MiniNN \n",
    "===\n",
    "\n",
    "Tutorial introdutório sobre Redes Neurais, proposto Lucas de Magalhães Araújo. O objetivo deste tutorial é mostrar o processo de treinamento em Redes Neurais mínimas, nas quais parâmetros (pesos) ótimos são conhecidos. A implementação deste tutorial foi feita usando framework [Keras](https://keras.io/).\n",
    "\n",
    "**Sua tarefa será encontrar parâmetros de treinamento de cada modelo de modo que eles aprendam!**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Como ajustar parâmetros de treinamento (hiperparâmetros)\n",
    "\n",
    "Chamamos de *hiperparâmetros* os parâmetros relacionados ao processo de treinamento da rede. Idealmente, queremos encontrar o conjunto de valores que convirjam a rede à configuração ótima no menor tempo possível. Abaixo, estão algumas sugestões de como realizar o ajuste destes hiperparâmetros. Estas sugestões são empíricas: encontrar o conjunto de hiperparâmetros que gera o resultado ótimo é um problema [NP-Difícil](https://en.wikipedia.org/wiki/NP-hardness). Uma estratégia geral é buscar valores em *espaço de log* - 1e1, 1e0, 1e-1, 1e-2 etc - e após realizar busca fina entre duas ordens de grandeza.\n",
    "\n",
    " - *Taxa de aprendizagem*: se o custo (loss) está oscilando ou aumentando, diminua a taxa de aprendizagem; se o custo está estático ou diminuindo muito lentamente, aumente a taxa de aprendizagem. \n",
    " - *Número de épocas*: uma *época* corresponde a uma passagem por todas as amostras do conjunto de treinamento. Se o custo ainda estiver decaindo, esta quantidade pode ser aumentada.\n",
    " - *Tamanho do batch*: o custo computacional da otimização cresce com o número de amostras. Na ordem de centenas de milhares ou milhões, pode se tornar impraticável. Uma estratégia é otimizar em *batches* (bateladas) ao invés de usar todo o conjunto de treinamento a cada iteração. Ademais, em muitos casos a convergência usando-se batches pequenos é mais rápida.\n",
    " - *Momento*: podemos pensar no momento como a *inércia* na descida de gradiente. Momento 0.0 implica em nenhuma inércia (ou máximo atrito na superfície de busca). Momento 1.0 implica em inércia total - sempre irá divergir para infinito (nenhum atrito na superfície de busca). Valores típicos são 0.0 (sem momento) ou no intervalo [0.9, 1.0).\n",
    " \n",
    " \n",
    "Outros aspectos que auxiliam na convergência já estão implementados (tipo de otimizador, função de ativação, método de inicialização dos pesos etc)\n",
    "\n",
    "**Observação**: o processo de convergência é sensível ao estado inicial da rede! Como os pesos iniciais são definidos aleatoriamente a cada execução, múltiplas tentativas com os mesmos hiperparâmetros podem gerar resultados drasticamente diferentes. Portanto, execute cada modelo algumas vezes e observe a variação nas respostas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importa bibliotecas e define funções auxiliares\n",
    "\n",
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "keras = tf.contrib.keras \n",
    "\n",
    "Sequential = tf.contrib.keras.models.Sequential\n",
    "Dense = tf.contrib.keras.layers.Dense\n",
    "Activation = tf.contrib.keras.layers.Activation\n",
    "Callback = tf.contrib.keras.callbacks.Callback\n",
    "\n",
    "def abs_distance(y_true, y_pred):\n",
    "    return np.abs(y_true-y_pred)\n",
    "\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset\n",
    "===\n",
    "\n",
    "Cria um dataset representando a relação de identidade y=x. Dataset contém 10.000 pontos no intervalo [-1000, 1000]\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 10000\n",
    "\n",
    "# Sorteia 10.000 pontos aleatórios no intervalo [0,1] com distribuição uniforme \n",
    "train_data = np.random.random(size=num_samples)\n",
    "# Reescala amostra para o intervalo [-1000, 1000]\n",
    "train_data = 2000 * train_data - 1000\n",
    "target = train_data.copy()\n",
    "\n",
    "# Plota pontos escolhidos\n",
    "plt.scatter(train_data,target, c='g', marker='.', alpha=0.7)\n",
    "plt.grid(True)\n",
    "plt.title(u'10.000 pontos da função identidade (y=x) no intervalo [-1000, 1000]')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ex. 0 - Regressor Linear\n",
    "===\n",
    "\n",
    "Este primeiro exemplo ajusta uma reta ao conjunto de treinamento. Obviamente, dado $y = ax + b$, esperamos encontrar $a = 1.0$ e $b = 0.0$ (identidade).\n",
    "\n",
    "Rode multiplas vezes este exemplo ajustando os parâmetros:\n",
    " - **learn_rate**: float $\\gt0$. Taxa de aprendizagem do otimizador;\n",
    " - **num_epochs**: int $\\ge1$. Número de épocas do treinamento (uma época equivale a passar por todas as amostras de treino);\n",
    " - **batch_size**: int $\\ge1, \\le$ tamanho do conjunto de treinamento. Tamanho do batch. Ex: se batch_size = 1000, então serão necessárias 10 iterações para percorrer uma época; se batch_size = 1, então serão necessárias 10000 iterações para percorrer uma época, etc.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *** PARÂMETROS ***\n",
    "learn_rate = 1e-2\n",
    "num_epochs = 10\n",
    "batch_size = 10000\n",
    "\n",
    "# ******************\n",
    "\n",
    "# Constrói o modelo. Uma camada densa sem função de ativação \n",
    "# equivale a um regressor linear!\n",
    "model = Sequential()\n",
    "model.add(Dense(1, input_dim=1, kernel_initializer='normal'))\n",
    "\n",
    "# Define o otimizador a ser usado. SGD - Stochastic Gradient Descent\n",
    "SGD = tf.contrib.keras.optimizers.SGD\n",
    "# Computa o decay a partir de decay_per_epoch\n",
    "optimizer = SGD(lr=learn_rate)\n",
    "\n",
    "# Combina o modelo e o otimizador. Como é um problema de regressão, \n",
    "# utiliza 'erro quadrático médio' como função de custo (função a ser minimizada)\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss='mean_squared_error',\n",
    "              metrics=[abs_distance])\n",
    "          \n",
    "# Mostra condição inicial do peso e bias\n",
    "print(\"Pré-treino:\")\n",
    "print(\" - peso:\", model.get_weights()[0][0][0])\n",
    "print(\" - bias:\", model.get_weights()[1][0])\n",
    "print()\n",
    "          \n",
    "# Realiza o treino\n",
    "model.fit(train_data, target, epochs=num_epochs, batch_size=batch_size)\n",
    "\n",
    "# Mostra peso e bias após treinamento\n",
    "print(\"\\nPós-treino:\")\n",
    "print(\" - peso:\", model.get_weights()[0][0][0])\n",
    "print(\" - bias:\", model.get_weights()[1][0])\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Teste**: escolha amostras de teste em diferentes ordens de grandeza e observe o erro. Qual a influência do *bias*?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Amostra de teste (escolha diferentes valores)\n",
    "test_sample = 1.000\n",
    "\n",
    "# Predição com o modelo treinado\n",
    "target_prediction = model.predict(np.array([test_sample]), batch_size=1)[0][0]\n",
    "\n",
    "print(\"Teste:\", test_sample)\n",
    "print(\"Predição:\", target_prediction)\n",
    "print(\"Erro absoluto:\", np.abs(target_prediction-test_sample))\n",
    "print(\"Erro percentual: %.4f%%\" % (np.abs(target_prediction-test_sample)/float(test_sample) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ex. 1 - Regressão com MiniNN\n",
    "===\n",
    "\n",
    "O próximo exemplo ajusta uma rede neural com somente dois neurônios ao conjunto de treinamento. Este é um problema de regressão, ou seja, dada uma entrada $x$, queremos estimar um alvo $y$ (neste caso, $y=x$). A arquitetura desta rede está representada no diagrama abaixo:\n",
    "\n",
    "![Arquitetura MiniNN - Regressor](MiniNN_Regressor.png)\n",
    "\n",
    "A função de ativação da camada escondida é a ReLU - Rectified Linear Unit, dada por:\n",
    "\n",
    "$ReLU(x) =\n",
    "\\left\\{\n",
    "\t\\begin{array}{ll}\n",
    "\t\tx  & \\mbox{if } x \\geq 0 \\\\\n",
    "\t\t0 & \\mbox{if } x < 0\n",
    "\t\\end{array}\n",
    "\\right.$\n",
    "\n",
    "Há infinitas soluções possíveis, porém uma simples é:\n",
    " * $w_0^0 = w_0^1 = 1$ (identifica o caminho positivo $x \\ge 0$ );\n",
    " * $w_1^0 = w_1^1 = -1$ (identifica o caminho negativo $x < 0$ );\n",
    " * $b_0^0 = b_0^1 = b_1 = 0$ (identidade não tem bias).\n",
    "\n",
    "Rode multiplas vezes este exemplo ajustando os parâmetros:\n",
    " - **learn_rate**: float $\\ge0$. Taxa de aprendizagem do otimizador;\n",
    " - **num_epochs**: int $\\ge1$. Número de épocas do treinamento;\n",
    " - **batch_size**: int $\\ge1, \\le$ tamanho do conjunto de treinamento. Tamanho do batch;\n",
    " - **momentum**: float $\\ge0$, $\\lt1$. Taxa do *momento* (\"inércia\") do otimizador. Ver explicação do funcionamento em http://ruder.io/optimizing-gradient-descent/index.html#momentum\n",
    " \n",
    "Observe como os pesos iniciais (definidos aleatoriamente a cada execução) afetam a resposta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *** PARÂMETROS ***\n",
    "learn_rate = 1e-2\n",
    "num_epochs = 10 \n",
    "batch_size = 10000\n",
    "momentum = 0.0\n",
    "\n",
    "# ******************\n",
    "\n",
    "# Constrói o modelo. Na primeira camada, usamos função de ativação ReLU\n",
    "model = Sequential()\n",
    "model.add(Dense(2, activation='relu', input_dim=1, kernel_initializer='normal'))\n",
    "model.add(Dense(1, kernel_initializer='normal'))\n",
    "\n",
    "\n",
    "# Define o otimizador a ser usado. SGD - Stochastic Gradient Descent\n",
    "SGD = tf.contrib.keras.optimizers.SGD\n",
    "# Computa o decay a partir de decay_per_epoch\n",
    "optimizer = SGD(lr=learn_rate, momentum=momentum, nesterov=True)\n",
    "\n",
    "# Combina o modelo e o otimizador. Como é um problema de regressão, \n",
    "# utiliza 'erro quadrático médio' como função de custo (função a ser minimizada)\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss='mean_squared_error',\n",
    "              metrics=[abs_distance])\n",
    "          \n",
    "# Mostra condição inicial do peso e bias\n",
    "print(\"Pré-treino:\")\n",
    "print(\" - pesos camada 0:\", model.get_weights()[0][0])\n",
    "print(\" - bias  camada 0:\", model.get_weights()[1])\n",
    "print(\" - pesos camada 1:\", model.get_weights()[2])\n",
    "print(\" - bias  camada 1:\", model.get_weights()[3])\n",
    "print()\n",
    "          \n",
    "# Realiza o treino\n",
    "model.fit(train_data, target, epochs=num_epochs, batch_size=batch_size)\n",
    "\n",
    "# Mostra peso e bias após treinamento\n",
    "print(\"\\nPós-treino:\")\n",
    "print(\" - pesos camada 0:\", model.get_weights()[0][0])\n",
    "print(\" - bias  camada 0:\", model.get_weights()[1])\n",
    "print(\" - pesos camada 1:\", model.get_weights()[2])\n",
    "print(\" - bias  camada 1:\", model.get_weights()[3])\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Teste**: escolha amostras de teste em diferentes ordens de grandeza e observe o erro. Note que o *caminho negativo* e o *caminho positivo* podem ter erros diferentes (por que?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Amostra de teste (escolha diferentes valores)\n",
    "test_sample = -1.0\n",
    "\n",
    "# Predição com o modelo treinado\n",
    "target_prediction = model.predict(np.array([test_sample]), batch_size=1)[0][0]\n",
    "\n",
    "print(\"Teste:\", test_sample)\n",
    "print(\"Predição:\", target_prediction)\n",
    "print(\"Erro absoluto:\", np.abs(target_prediction-test_sample))\n",
    "print(\"Erro percentual: %.4f%%\" % (np.abs(target_prediction-test_sample)/float(test_sample) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ex. 2 - Classificação com MiniNN\n",
    "===\n",
    "\n",
    "Este exemplo mapeia uma rede com dois neurônios para a operação lógica XOR (ou exclusivo). Esta operação pode ser definida como\n",
    "\n",
    "$XOR(x_0, x_1) =\n",
    "\\left\\{\n",
    "\t\\begin{array}{ll}\n",
    "\t\t1 \\space(true) & \\mbox{if } x_0\\neq x_1 \\\\\n",
    "\t\t0 \\space(false) & \\mbox{if } x_0=x_1\n",
    "\t\\end{array}\n",
    "\\right.$\n",
    "\n",
    "Este é um problema de classificação, ou seja, dadas entradas $x_0, x_1$, queremos classificar a entrada como verdadeira ou falsa. Este é um exemplo mínimo de relação não-linear, ou seja, não há uma reta que consiga fazer a separação das classes.\n",
    "\n",
    "A arquitetura desta rede está representada no diagrama abaixo:\n",
    "\n",
    "![Arquitetura MiniNN - Classificador](MiniNN_Classification.png)\n",
    "\n",
    "A função de ativação da camada escondida é a [Sigmóide](https://pt.wikipedia.org/wiki/Fun%C3%A7%C3%A3o_sigm%C3%B3ide), dada por\n",
    "\n",
    "$\\Sigma (z) = {\\large \\frac{1}{1+e^{-z}}}$\n",
    "\n",
    "onde $z$ é combinação linear dos elementos da camanda anterior. A saída da função sigmóide é *contínua* no intervalo $(0,1)$, porém podemos estabelecer um *limiar* $T$ tal que a resposta $y$ da rede será $0$, se $y\\le T$ e $1$, se $y > T$. Um limiar típico seria $T=0.5$.\n",
    "\n",
    "**Exercício**: encontre conjunto adequado de pesos e biases para que esta Rede Neural expresse a relação XOR. *(A última célula deste Notebook oferece uma resposta possível)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria dataset contendo a relação XOR\n",
    "\n",
    "train_data = [[0.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 1.0]]\n",
    "train_data = np.array(train_data, dtype=np.float32)\n",
    "labels = [[0.0], [1.0], [1.0], [0.0]]\n",
    "labels = np.array(labels)\n",
    "\n",
    "# Plota pontos escolhidos\n",
    "fig = plt.figure(num=None, figsize=(3.5, 3.5), dpi=80, facecolor='w', edgecolor='k')\n",
    "plt.scatter(train_data[1],train_data[2], c='b', marker='o', s=400)\n",
    "plt.scatter(train_data[2],train_data[2], c='r', marker='o', s=400)\n",
    "plt.grid(True)\n",
    "plt.locator_params(nbins=2)\n",
    "plt.title(u'XOR')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rode multiplas vezes este exemplo ajustando os parâmetros:\n",
    " - **learn_rate**: float $\\ge0$. Taxa de aprendizagem do otimizador;\n",
    " - **num_epochs**: int $\\ge1$. Número de épocas do treinamento;\n",
    " - **batch_size**: int $\\ge1, \\le$ tamanho do conjunto de treinamento. Tamanho do batch;\n",
    " - **momentum**: float $\\ge0$, $\\lt1$. Taxa do *momento* (\"inércia\") do otimizador.\n",
    "\n",
    "**Obs**: Durante o treinamento, a informação *acc* se refere à acurácia do modelo. Para este modelo, podemos ter acc $\\in$ {0%, 25%, 50%, 75%, 100%} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *** PARÂMETROS ***\n",
    "learn_rate = 1e-2\n",
    "num_epochs = 10\n",
    "batch_size = 4\n",
    "momentum = 0.0\n",
    "\n",
    "# ******************\n",
    "\n",
    "# Constrói o modelo. Na primeira camada, usamos função de ativação ReLU\n",
    "initializer = keras.initializers.RandomUniform(minval=-1.00, maxval=1.00, seed=None)\n",
    "model = Sequential()\n",
    "model.add(Dense(2, activation='sigmoid', input_dim=2, kernel_initializer=initializer))\n",
    "# Descomente a linha abaixo para adicionar uma camada escondida\n",
    "#model.add(Dense(2, activation='sigmoid', kernel_initializer=initializer))\n",
    "model.add(Dense(1, activation='sigmoid', kernel_initializer=initializer))\n",
    "\n",
    "\n",
    "# Define o otimizador a ser usado. SGD - Stochastic Gradient Descent\n",
    "SGD = tf.contrib.keras.optimizers.SGD\n",
    "# Computa o decay a partir de decay_per_epoch\n",
    "optimizer = SGD(lr=learn_rate, momentum=momentum, nesterov=True)\n",
    "\n",
    "# Combina o modelo e o otimizador. Como é um problema de regressão, \n",
    "# utiliza 'erro quadrático médio' como função de custo (função a ser minimizada)\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "          \n",
    "# Realiza o treino\n",
    "model.fit(train_data, labels, epochs=num_epochs, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predição com o modelo treinado\n",
    "prediction = model.predict(train_data, batch_size=4)\n",
    "\n",
    "loss = 0\n",
    "print(\"X1\\tX2\\tY\")\n",
    "for i in range(4):\n",
    "    print(\"%.1f\\t%.1f\\t%.3f\" % (train_data[i,0], train_data[i,1], prediction[i])) \n",
    "    loss += abs_distance(labels[i], prediction[i])\n",
    "\n",
    "print(\"\\nDistância L1:\", loss[0]/4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Curiosamente difícil conseguir a convergência! O paper [Learning XOR: exploring the space of a classic problem](http://yen.cs.stir.ac.uk/~kjt/techreps/pdf/TR148.pdf) faz uma exploração e explicação aprofundada da dificuldade em convergir uma Rede Neural ao problema XOR. Por alto, há diversos mínimos locais que podem interromper a convergência ao mínimo global. Este problema é especialmente sensível ao estado inicial da rede.\n",
    "\n",
    "Abaixo, temos uma possível resposta para os pesos e biases da rede"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rede Neural com os pesos pré-definidos que computa a operação XOR\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1.0 / (1 + np.exp(-z))\n",
    "\n",
    "def xor(x):\n",
    "    x0 = x[0]\n",
    "    x1 = x[1]\n",
    "    h0 = sigmoid(100*x0 - 100*x1 - 50) # X0 and not X1\n",
    "    h1 = sigmoid(100*x1 - 100*x0 - 50) # not X0 and X1\n",
    "    return sigmoid(100*h0 + 100*h1 - 50) # H0 or H1\n",
    "\n",
    "loss = 0\n",
    "print(\"X1\\tX2\\tY\")\n",
    "for i in range(4):\n",
    "    print(\"%.1f\\t%.1f\\t%.3f\" % (train_data[i,0], train_data[i,1], xor(train_data[i]))) \n",
    "    loss += abs_distance(labels[i], xor(train_data[i]))\n",
    "\n",
    "print(\"\\nDistância L1:\", loss[0]/4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
